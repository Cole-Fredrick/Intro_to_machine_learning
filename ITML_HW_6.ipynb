{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b7e4ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9b7e4ab",
        "outputId": "2dab0dd8-c39f-4a62-8ee1-ebfbe2af92bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8bba4faf90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc21268",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adc21268",
        "outputId": "ebf0b918-a695-4383-9876-75547e384135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a9214f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "55fda10dd22042d08466bb4ee0e516b9",
            "46ef21000ec94c45856f39dceb4b8aef",
            "1b2c4106dd88477cbbd39ac3654de724",
            "b33fe9614f534ef28c002a2b09ebbb78",
            "c524a45ee05846eb9018ae78add1b0cf",
            "9f6544e89cbd444bbb843d26ca18a045",
            "c3bca4c09b6e4ed0a2d519bea7550e42",
            "372c6a058c3a484c9af8acbbe5738862",
            "7ce5a74ea39d4c469f54d0bfb8746023",
            "9c18b4b364924a5d913a19afa3f54413",
            "e88e584e07af4187a81e1c4973d99b79"
          ]
        },
        "id": "b5a9214f",
        "outputId": "69a7dab6-efd5-40ac-b660-a3f560ae6783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55fda10dd22042d08466bb4ee0e516b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1 (50 pts):**\n",
        "\n",
        "**1.a. Create a fully connected Neural Network for all 10 classes in CIFAR-10 with only one hidden layer with the size of 512. Train your network for 300 epochs. Report your training time, training loss and evaluation accuracy after 300 epochs. Analyze your results in your report. Make sure to submit your code by providing the GitHub URL of your course repository for this course. (20pt)**"
      ],
      "metadata": {
        "id": "6hpAmbxZfqwR"
      },
      "id": "6hpAmbxZfqwR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd0d29f",
      "metadata": {
        "id": "bfd0d29f"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c208fee",
      "metadata": {
        "id": "9c208fee"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_train += loss.item()\n",
        "    print('{} Epoch {}, Training loss {}'.format(\n",
        "    datetime.datetime.now(), epoch,\n",
        "    loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "          imgs, labels = imgs.to(device), labels.to(device)\n",
        "          batchsize = imgs.shape[0]\n",
        "          outputs = model(imgs)\n",
        "          _, predicted = torch.max(outputs, dim=1)\n",
        "          total += labels.shape[0]\n",
        "          correct += int((predicted == labels).sum())\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
      ],
      "metadata": {
        "id": "MfUQIIfYmKdm"
      },
      "id": "MfUQIIfYmKdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75714be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75714be4",
        "outputId": "20a172a8-5cb8-4d8c-94cd-83cb658f4a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-29 02:55:28.511502 Epoch 1, Training loss 2.098292909467312\n",
            "2022-03-29 02:55:44.957787 Epoch 2, Training loss 1.785247363855162\n",
            "2022-03-29 02:56:01.498808 Epoch 3, Training loss 1.6172291255363114\n",
            "2022-03-29 02:56:18.106240 Epoch 4, Training loss 1.5265314772610774\n",
            "2022-03-29 02:56:34.565004 Epoch 5, Training loss 1.460617965901904\n",
            "2022-03-29 02:56:51.125967 Epoch 6, Training loss 1.3960864796967762\n",
            "2022-03-29 02:57:07.374479 Epoch 7, Training loss 1.3315046275668132\n",
            "2022-03-29 02:57:23.676630 Epoch 8, Training loss 1.2779924499866602\n",
            "2022-03-29 02:57:40.115119 Epoch 9, Training loss 1.2345515409546435\n",
            "2022-03-29 02:57:56.530338 Epoch 10, Training loss 1.1979004371044275\n",
            "2022-03-29 02:58:13.287508 Epoch 11, Training loss 1.1683111435464582\n",
            "2022-03-29 02:58:30.434091 Epoch 12, Training loss 1.1412167190895666\n",
            "2022-03-29 02:58:47.213326 Epoch 13, Training loss 1.1183172301258273\n",
            "2022-03-29 02:59:05.539046 Epoch 14, Training loss 1.0987535007774372\n",
            "2022-03-29 02:59:23.375201 Epoch 15, Training loss 1.0813645399592418\n",
            "2022-03-29 02:59:39.743912 Epoch 16, Training loss 1.064727510104094\n",
            "2022-03-29 02:59:56.237525 Epoch 17, Training loss 1.0497497331608288\n",
            "2022-03-29 03:00:12.552136 Epoch 18, Training loss 1.0367035318518538\n",
            "2022-03-29 03:00:28.910718 Epoch 19, Training loss 1.0225555322054403\n",
            "2022-03-29 03:00:45.144734 Epoch 20, Training loss 1.0104577020001229\n",
            "2022-03-29 03:01:01.272248 Epoch 21, Training loss 0.9984980390962127\n",
            "2022-03-29 03:01:17.389896 Epoch 22, Training loss 0.9870203161026205\n",
            "2022-03-29 03:01:33.446773 Epoch 23, Training loss 0.9763542122548193\n",
            "2022-03-29 03:01:49.535846 Epoch 24, Training loss 0.9685943522264281\n",
            "2022-03-29 03:02:05.608171 Epoch 25, Training loss 0.9599722945476736\n",
            "2022-03-29 03:02:21.853665 Epoch 26, Training loss 0.9513908764133063\n",
            "2022-03-29 03:02:37.972432 Epoch 27, Training loss 0.9430117405893857\n",
            "2022-03-29 03:02:53.965290 Epoch 28, Training loss 0.9361549005331591\n",
            "2022-03-29 03:03:10.093494 Epoch 29, Training loss 0.9279146470377208\n",
            "2022-03-29 03:03:26.541258 Epoch 30, Training loss 0.9211312612456739\n",
            "2022-03-29 03:03:42.834169 Epoch 31, Training loss 0.9158164428932892\n",
            "2022-03-29 03:03:59.097944 Epoch 32, Training loss 0.9086433960043866\n",
            "2022-03-29 03:04:15.396773 Epoch 33, Training loss 0.9024460647264709\n",
            "2022-03-29 03:04:31.594407 Epoch 34, Training loss 0.8983353927464741\n",
            "2022-03-29 03:04:48.076073 Epoch 35, Training loss 0.8920338855070227\n",
            "2022-03-29 03:05:04.449025 Epoch 36, Training loss 0.8878011528183433\n",
            "2022-03-29 03:05:20.634813 Epoch 37, Training loss 0.8808134042698404\n",
            "2022-03-29 03:05:36.799275 Epoch 38, Training loss 0.8751728677231333\n",
            "2022-03-29 03:05:52.888188 Epoch 39, Training loss 0.8715454747762217\n",
            "2022-03-29 03:06:09.337100 Epoch 40, Training loss 0.8678278495436129\n",
            "2022-03-29 03:06:26.099361 Epoch 41, Training loss 0.8627779941882014\n",
            "2022-03-29 03:06:42.244368 Epoch 42, Training loss 0.8587816173920546\n",
            "2022-03-29 03:06:58.322610 Epoch 43, Training loss 0.8532133981623613\n",
            "2022-03-29 03:07:14.402082 Epoch 44, Training loss 0.8491877326956185\n",
            "2022-03-29 03:07:30.517585 Epoch 45, Training loss 0.8461499280484436\n",
            "2022-03-29 03:07:46.731280 Epoch 46, Training loss 0.8419598073453245\n",
            "2022-03-29 03:08:03.126595 Epoch 47, Training loss 0.8397845892650088\n",
            "2022-03-29 03:08:19.691139 Epoch 48, Training loss 0.833157032118429\n",
            "2022-03-29 03:08:36.184989 Epoch 49, Training loss 0.830636063042809\n",
            "2022-03-29 03:08:52.691442 Epoch 50, Training loss 0.8281766366394584\n",
            "2022-03-29 03:09:08.840545 Epoch 51, Training loss 0.8235505400868632\n",
            "2022-03-29 03:09:25.040153 Epoch 52, Training loss 0.8206282400185495\n",
            "2022-03-29 03:09:41.121176 Epoch 53, Training loss 0.81739061819318\n",
            "2022-03-29 03:09:57.273842 Epoch 54, Training loss 0.8134695308668839\n",
            "2022-03-29 03:10:13.408780 Epoch 55, Training loss 0.8097742314228926\n",
            "2022-03-29 03:10:29.530875 Epoch 56, Training loss 0.8083909700822343\n",
            "2022-03-29 03:10:45.591301 Epoch 57, Training loss 0.8045051425238094\n",
            "2022-03-29 03:11:01.537537 Epoch 58, Training loss 0.8031258754565588\n",
            "2022-03-29 03:11:17.721693 Epoch 59, Training loss 0.798725777155603\n",
            "2022-03-29 03:11:33.830301 Epoch 60, Training loss 0.7961614345345656\n",
            "2022-03-29 03:11:50.044405 Epoch 61, Training loss 0.7920944979199973\n",
            "2022-03-29 03:12:06.192294 Epoch 62, Training loss 0.7891762294351597\n",
            "2022-03-29 03:12:22.324718 Epoch 63, Training loss 0.7876228921477447\n",
            "2022-03-29 03:12:38.386841 Epoch 64, Training loss 0.7827830054723394\n",
            "2022-03-29 03:12:54.430643 Epoch 65, Training loss 0.7805662662781718\n",
            "2022-03-29 03:13:10.491849 Epoch 66, Training loss 0.779191897272149\n",
            "2022-03-29 03:13:26.584049 Epoch 67, Training loss 0.7763667689717334\n",
            "2022-03-29 03:13:42.757740 Epoch 68, Training loss 0.7745497886024778\n",
            "2022-03-29 03:13:58.939717 Epoch 69, Training loss 0.7702815239996557\n",
            "2022-03-29 03:14:15.142775 Epoch 70, Training loss 0.7677711191613351\n",
            "2022-03-29 03:14:31.360705 Epoch 71, Training loss 0.7674367167913091\n",
            "2022-03-29 03:14:47.484668 Epoch 72, Training loss 0.7642256206716113\n",
            "2022-03-29 03:15:03.656266 Epoch 73, Training loss 0.7625424952991783\n",
            "2022-03-29 03:15:19.730432 Epoch 74, Training loss 0.7589570281984251\n",
            "2022-03-29 03:15:35.778328 Epoch 75, Training loss 0.7567437870042099\n",
            "2022-03-29 03:15:51.842845 Epoch 76, Training loss 0.7556332957637889\n",
            "2022-03-29 03:16:07.886566 Epoch 77, Training loss 0.7522985399379145\n",
            "2022-03-29 03:16:23.948217 Epoch 78, Training loss 0.7496131711908619\n",
            "2022-03-29 03:16:39.991489 Epoch 79, Training loss 0.7471572187398096\n",
            "2022-03-29 03:16:56.041072 Epoch 80, Training loss 0.7449337906773438\n",
            "2022-03-29 03:17:11.943535 Epoch 81, Training loss 0.742473365667531\n",
            "2022-03-29 03:17:27.812429 Epoch 82, Training loss 0.7424075911417032\n",
            "2022-03-29 03:17:43.701024 Epoch 83, Training loss 0.7392076789723028\n",
            "2022-03-29 03:17:59.676583 Epoch 84, Training loss 0.7369455094532589\n",
            "2022-03-29 03:18:15.622429 Epoch 85, Training loss 0.7350164632053326\n",
            "2022-03-29 03:18:31.623503 Epoch 86, Training loss 0.7323505903220238\n",
            "2022-03-29 03:18:47.622012 Epoch 87, Training loss 0.731219464288953\n",
            "2022-03-29 03:19:03.532690 Epoch 88, Training loss 0.7283291466476972\n",
            "2022-03-29 03:19:19.574703 Epoch 89, Training loss 0.7261771639365979\n",
            "2022-03-29 03:19:35.572594 Epoch 90, Training loss 0.7251748076027922\n",
            "2022-03-29 03:19:51.595220 Epoch 91, Training loss 0.7225163981432805\n",
            "2022-03-29 03:20:07.699194 Epoch 92, Training loss 0.7206250161999632\n",
            "2022-03-29 03:20:23.932318 Epoch 93, Training loss 0.7192150644786522\n",
            "2022-03-29 03:20:39.982688 Epoch 94, Training loss 0.716696811606512\n",
            "2022-03-29 03:20:56.259141 Epoch 95, Training loss 0.7169016753621114\n",
            "2022-03-29 03:21:12.465735 Epoch 96, Training loss 0.7133899548703142\n",
            "2022-03-29 03:21:28.628094 Epoch 97, Training loss 0.7120571545017954\n",
            "2022-03-29 03:21:44.818767 Epoch 98, Training loss 0.7093997961267486\n",
            "2022-03-29 03:22:00.992022 Epoch 99, Training loss 0.7083234695141273\n",
            "2022-03-29 03:22:17.167209 Epoch 100, Training loss 0.7069524261728882\n",
            "2022-03-29 03:22:33.415375 Epoch 101, Training loss 0.704463880652052\n",
            "2022-03-29 03:22:49.652089 Epoch 102, Training loss 0.702961806522306\n",
            "2022-03-29 03:23:05.894280 Epoch 103, Training loss 0.7021463707737301\n",
            "2022-03-29 03:23:22.133187 Epoch 104, Training loss 0.6982278471331462\n",
            "2022-03-29 03:23:38.386034 Epoch 105, Training loss 0.6973925447829848\n",
            "2022-03-29 03:23:54.706446 Epoch 106, Training loss 0.695757226489694\n",
            "2022-03-29 03:24:11.019403 Epoch 107, Training loss 0.6966190300405483\n",
            "2022-03-29 03:24:27.263199 Epoch 108, Training loss 0.6935375241748513\n",
            "2022-03-29 03:24:43.608963 Epoch 109, Training loss 0.6918945741622954\n",
            "2022-03-29 03:24:59.984855 Epoch 110, Training loss 0.6908204024252684\n",
            "2022-03-29 03:25:16.390987 Epoch 111, Training loss 0.6880581767281608\n",
            "2022-03-29 03:25:32.806778 Epoch 112, Training loss 0.6884850618403281\n",
            "2022-03-29 03:25:49.115315 Epoch 113, Training loss 0.6862342139262982\n",
            "2022-03-29 03:26:05.470580 Epoch 114, Training loss 0.6833256302815874\n",
            "2022-03-29 03:26:21.884349 Epoch 115, Training loss 0.6833944377082083\n",
            "2022-03-29 03:26:38.262657 Epoch 116, Training loss 0.6817905578924262\n",
            "2022-03-29 03:26:54.852197 Epoch 117, Training loss 0.6803156804017094\n",
            "2022-03-29 03:27:11.249771 Epoch 118, Training loss 0.6785744370325751\n",
            "2022-03-29 03:27:27.584306 Epoch 119, Training loss 0.6768169121821518\n",
            "2022-03-29 03:27:43.979032 Epoch 120, Training loss 0.676035494755601\n",
            "2022-03-29 03:28:00.404693 Epoch 121, Training loss 0.672240778079728\n",
            "2022-03-29 03:28:16.786537 Epoch 122, Training loss 0.6738973774797167\n",
            "2022-03-29 03:28:32.949087 Epoch 123, Training loss 0.6711863099080523\n",
            "2022-03-29 03:28:49.162716 Epoch 124, Training loss 0.6698969430325891\n",
            "2022-03-29 03:29:05.403201 Epoch 125, Training loss 0.6681691270578852\n",
            "2022-03-29 03:29:21.702528 Epoch 126, Training loss 0.6673993117669049\n",
            "2022-03-29 03:29:38.085400 Epoch 127, Training loss 0.6634987436444558\n",
            "2022-03-29 03:29:54.411553 Epoch 128, Training loss 0.6661889236372756\n",
            "2022-03-29 03:30:10.772222 Epoch 129, Training loss 0.6635072588768152\n",
            "2022-03-29 03:30:27.104590 Epoch 130, Training loss 0.6614431406149779\n",
            "2022-03-29 03:30:43.473272 Epoch 131, Training loss 0.660283566778883\n",
            "2022-03-29 03:30:59.801192 Epoch 132, Training loss 0.660198241357913\n",
            "2022-03-29 03:31:16.136785 Epoch 133, Training loss 0.6599525939244444\n",
            "2022-03-29 03:31:32.443337 Epoch 134, Training loss 0.6585558965764082\n",
            "2022-03-29 03:31:48.725654 Epoch 135, Training loss 0.6552081508252322\n",
            "2022-03-29 03:32:05.119104 Epoch 136, Training loss 0.6551416105855151\n",
            "2022-03-29 03:32:21.466180 Epoch 137, Training loss 0.6542735538824135\n",
            "2022-03-29 03:32:37.726943 Epoch 138, Training loss 0.6519401679410959\n",
            "2022-03-29 03:32:54.043873 Epoch 139, Training loss 0.6510785673280506\n",
            "2022-03-29 03:33:10.313799 Epoch 140, Training loss 0.649881843182132\n",
            "2022-03-29 03:33:26.662131 Epoch 141, Training loss 0.6478022770274936\n",
            "2022-03-29 03:33:42.928730 Epoch 142, Training loss 0.6479237203479118\n",
            "2022-03-29 03:33:59.174322 Epoch 143, Training loss 0.6477943838328657\n",
            "2022-03-29 03:34:15.503639 Epoch 144, Training loss 0.6471176811724978\n",
            "2022-03-29 03:34:31.855183 Epoch 145, Training loss 0.6452394402240549\n",
            "2022-03-29 03:34:48.043390 Epoch 146, Training loss 0.6447957874945057\n",
            "2022-03-29 03:35:04.402604 Epoch 147, Training loss 0.6411082331862901\n",
            "2022-03-29 03:35:20.758419 Epoch 148, Training loss 0.6404653158791535\n",
            "2022-03-29 03:35:37.266283 Epoch 149, Training loss 0.6404137876256347\n",
            "2022-03-29 03:35:53.715032 Epoch 150, Training loss 0.6381602049864772\n",
            "2022-03-29 03:36:09.998775 Epoch 151, Training loss 0.6364582180595764\n",
            "2022-03-29 03:36:26.309118 Epoch 152, Training loss 0.6371242385309981\n",
            "2022-03-29 03:36:42.560236 Epoch 153, Training loss 0.6359944764686667\n",
            "2022-03-29 03:36:58.876718 Epoch 154, Training loss 0.6354844647905101\n",
            "2022-03-29 03:37:15.106266 Epoch 155, Training loss 0.6345104941397982\n",
            "2022-03-29 03:37:31.398395 Epoch 156, Training loss 0.6334240452179214\n",
            "2022-03-29 03:37:47.629036 Epoch 157, Training loss 0.6315438544277645\n",
            "2022-03-29 03:38:03.743361 Epoch 158, Training loss 0.6302390700334783\n",
            "2022-03-29 03:38:20.098181 Epoch 159, Training loss 0.6299181064147779\n",
            "2022-03-29 03:38:36.438430 Epoch 160, Training loss 0.6297976002287682\n",
            "2022-03-29 03:38:52.677255 Epoch 161, Training loss 0.628501235142998\n",
            "2022-03-29 03:39:09.079665 Epoch 162, Training loss 0.6275027025386196\n",
            "2022-03-29 03:39:25.345492 Epoch 163, Training loss 0.6259408874813553\n",
            "2022-03-29 03:39:41.628661 Epoch 164, Training loss 0.6250030369404942\n",
            "2022-03-29 03:39:58.309286 Epoch 165, Training loss 0.6230544558418986\n",
            "2022-03-29 03:40:14.841232 Epoch 166, Training loss 0.6215358815915749\n",
            "2022-03-29 03:40:31.406809 Epoch 167, Training loss 0.6207286810783474\n",
            "2022-03-29 03:40:47.702196 Epoch 168, Training loss 0.6209018302466863\n",
            "2022-03-29 03:41:04.028877 Epoch 169, Training loss 0.6204132209043673\n",
            "2022-03-29 03:41:20.311409 Epoch 170, Training loss 0.6198986939075962\n",
            "2022-03-29 03:41:36.585813 Epoch 171, Training loss 0.6173397844938366\n",
            "2022-03-29 03:41:52.895537 Epoch 172, Training loss 0.6177224553835666\n",
            "2022-03-29 03:42:09.118998 Epoch 173, Training loss 0.6172530971600881\n",
            "2022-03-29 03:42:25.491747 Epoch 174, Training loss 0.6180217169663486\n",
            "2022-03-29 03:42:41.746891 Epoch 175, Training loss 0.6145943024045671\n",
            "2022-03-29 03:42:58.031072 Epoch 176, Training loss 0.6143064281672163\n",
            "2022-03-29 03:43:14.383184 Epoch 177, Training loss 0.6124090248209131\n",
            "2022-03-29 03:43:30.850397 Epoch 178, Training loss 0.6116840524213089\n",
            "2022-03-29 03:43:47.359972 Epoch 179, Training loss 0.6121559850776287\n",
            "2022-03-29 03:44:03.758293 Epoch 180, Training loss 0.6103364800476967\n",
            "2022-03-29 03:44:20.145420 Epoch 181, Training loss 0.6104684246470556\n",
            "2022-03-29 03:44:36.511899 Epoch 182, Training loss 0.6101199682716214\n",
            "2022-03-29 03:44:52.846720 Epoch 183, Training loss 0.6076380944694094\n",
            "2022-03-29 03:45:09.123660 Epoch 184, Training loss 0.6048304301012507\n",
            "2022-03-29 03:45:25.326196 Epoch 185, Training loss 0.6058011298518047\n",
            "2022-03-29 03:45:41.653638 Epoch 186, Training loss 0.6053872532246972\n",
            "2022-03-29 03:45:57.992787 Epoch 187, Training loss 0.6037892896653442\n",
            "2022-03-29 03:46:14.174101 Epoch 188, Training loss 0.6044694313688961\n",
            "2022-03-29 03:46:30.382026 Epoch 189, Training loss 0.6030095000096294\n",
            "2022-03-29 03:46:46.631875 Epoch 190, Training loss 0.6021149679065665\n",
            "2022-03-29 03:47:02.906864 Epoch 191, Training loss 0.6028771602055606\n",
            "2022-03-29 03:47:19.174717 Epoch 192, Training loss 0.6013981091701771\n",
            "2022-03-29 03:47:35.340549 Epoch 193, Training loss 0.5986585592293678\n",
            "2022-03-29 03:47:51.532116 Epoch 194, Training loss 0.5975984423361775\n",
            "2022-03-29 03:48:07.744541 Epoch 195, Training loss 0.5978435220011055\n",
            "2022-03-29 03:48:24.069693 Epoch 196, Training loss 0.5993723526520802\n",
            "2022-03-29 03:48:40.306052 Epoch 197, Training loss 0.5968530825946642\n",
            "2022-03-29 03:48:56.514294 Epoch 198, Training loss 0.594723846182189\n",
            "2022-03-29 03:49:12.749465 Epoch 199, Training loss 0.5936594675950078\n",
            "2022-03-29 03:49:28.921471 Epoch 200, Training loss 0.5956517864218758\n",
            "2022-03-29 03:49:45.224975 Epoch 201, Training loss 0.5949522890436375\n",
            "2022-03-29 03:50:01.528610 Epoch 202, Training loss 0.5931381398759534\n",
            "2022-03-29 03:50:17.724110 Epoch 203, Training loss 0.5918398264728849\n",
            "2022-03-29 03:50:33.882056 Epoch 204, Training loss 0.5918220973685574\n",
            "2022-03-29 03:50:50.084213 Epoch 205, Training loss 0.5917728973928925\n",
            "2022-03-29 03:51:06.290383 Epoch 206, Training loss 0.5889557534662049\n",
            "2022-03-29 03:51:22.564670 Epoch 207, Training loss 0.589007810024959\n",
            "2022-03-29 03:51:38.693319 Epoch 208, Training loss 0.5887828133523921\n",
            "2022-03-29 03:51:54.824733 Epoch 209, Training loss 0.5879510962368583\n",
            "2022-03-29 03:52:10.964859 Epoch 210, Training loss 0.5869513181469325\n",
            "2022-03-29 03:52:27.074681 Epoch 211, Training loss 0.5873901837164789\n",
            "2022-03-29 03:52:43.185970 Epoch 212, Training loss 0.5860331119097713\n",
            "2022-03-29 03:52:59.356420 Epoch 213, Training loss 0.5850086811253482\n",
            "2022-03-29 03:53:15.547781 Epoch 214, Training loss 0.5845975238267723\n",
            "2022-03-29 03:53:31.674232 Epoch 215, Training loss 0.5843934837890707\n",
            "2022-03-29 03:53:47.843441 Epoch 216, Training loss 0.5823584169035068\n",
            "2022-03-29 03:54:04.004052 Epoch 217, Training loss 0.5803724115385729\n",
            "2022-03-29 03:54:20.237305 Epoch 218, Training loss 0.5823202370225317\n",
            "2022-03-29 03:54:36.351149 Epoch 219, Training loss 0.5810132163107548\n",
            "2022-03-29 03:54:52.567086 Epoch 220, Training loss 0.5801094160284228\n",
            "2022-03-29 03:55:08.710967 Epoch 221, Training loss 0.5817040011782171\n",
            "2022-03-29 03:55:25.033534 Epoch 222, Training loss 0.5797656889614242\n",
            "2022-03-29 03:55:41.486116 Epoch 223, Training loss 0.5784043604531861\n",
            "2022-03-29 03:55:57.990513 Epoch 224, Training loss 0.5766531289995783\n",
            "2022-03-29 03:56:14.075856 Epoch 225, Training loss 0.5766737146107742\n",
            "2022-03-29 03:56:30.148259 Epoch 226, Training loss 0.5764557833180708\n",
            "2022-03-29 03:56:46.350353 Epoch 227, Training loss 0.5766026846054569\n",
            "2022-03-29 03:57:02.578112 Epoch 228, Training loss 0.5764595542455573\n",
            "2022-03-29 03:57:19.140830 Epoch 229, Training loss 0.5737152095036129\n",
            "2022-03-29 03:57:35.605358 Epoch 230, Training loss 0.5730849945407999\n",
            "2022-03-29 03:57:51.961391 Epoch 231, Training loss 0.5734220488983042\n",
            "2022-03-29 03:58:08.317941 Epoch 232, Training loss 0.5722536023544229\n",
            "2022-03-29 03:58:24.648248 Epoch 233, Training loss 0.5721614770877087\n",
            "2022-03-29 03:58:40.832469 Epoch 234, Training loss 0.572029745296749\n",
            "2022-03-29 03:58:56.954776 Epoch 235, Training loss 0.5708935175405438\n",
            "2022-03-29 03:59:13.134791 Epoch 236, Training loss 0.5707397812696369\n",
            "2022-03-29 03:59:29.454271 Epoch 237, Training loss 0.5694762440898534\n",
            "2022-03-29 03:59:45.620578 Epoch 238, Training loss 0.5697324940996706\n",
            "2022-03-29 04:00:02.014775 Epoch 239, Training loss 0.5680903096485626\n",
            "2022-03-29 04:00:18.692885 Epoch 240, Training loss 0.5681627970141219\n",
            "2022-03-29 04:00:34.912221 Epoch 241, Training loss 0.5686214571947332\n",
            "2022-03-29 04:00:51.005272 Epoch 242, Training loss 0.5662955959587146\n",
            "2022-03-29 04:01:07.268099 Epoch 243, Training loss 0.5646733320734995\n",
            "2022-03-29 04:01:23.499576 Epoch 244, Training loss 0.5651377477609288\n",
            "2022-03-29 04:01:39.619141 Epoch 245, Training loss 0.5652412935672209\n",
            "2022-03-29 04:01:55.898391 Epoch 246, Training loss 0.5643784483832777\n",
            "2022-03-29 04:02:12.083803 Epoch 247, Training loss 0.5635640635286145\n",
            "2022-03-29 04:02:28.274516 Epoch 248, Training loss 0.5635654547673357\n",
            "2022-03-29 04:02:44.566671 Epoch 249, Training loss 0.5618987839545131\n",
            "2022-03-29 04:03:00.848153 Epoch 250, Training loss 0.5629629966852915\n",
            "2022-03-29 04:03:17.288958 Epoch 251, Training loss 0.5617856079583887\n",
            "2022-03-29 04:03:33.593402 Epoch 252, Training loss 0.5608622217193588\n",
            "2022-03-29 04:03:49.947362 Epoch 253, Training loss 0.5597834090899934\n",
            "2022-03-29 04:04:06.310315 Epoch 254, Training loss 0.5595557119916467\n",
            "2022-03-29 04:04:22.541591 Epoch 255, Training loss 0.5595403174152764\n",
            "2022-03-29 04:04:38.669207 Epoch 256, Training loss 0.5597032622608078\n",
            "2022-03-29 04:04:54.924373 Epoch 257, Training loss 0.5584822267370151\n",
            "2022-03-29 04:05:11.088412 Epoch 258, Training loss 0.5578903530717201\n",
            "2022-03-29 04:05:27.254112 Epoch 259, Training loss 0.557936127304726\n",
            "2022-03-29 04:05:43.420444 Epoch 260, Training loss 0.5555135716334023\n",
            "2022-03-29 04:05:59.557690 Epoch 261, Training loss 0.5560582029011548\n",
            "2022-03-29 04:06:15.683631 Epoch 262, Training loss 0.5546902722257483\n",
            "2022-03-29 04:06:31.754618 Epoch 263, Training loss 0.5547028680134307\n",
            "2022-03-29 04:06:47.836845 Epoch 264, Training loss 0.553103031023689\n",
            "2022-03-29 04:07:03.915274 Epoch 265, Training loss 0.5528947881344334\n",
            "2022-03-29 04:07:20.183815 Epoch 266, Training loss 0.5543682788262891\n",
            "2022-03-29 04:07:36.547501 Epoch 267, Training loss 0.5534831987470007\n",
            "2022-03-29 04:07:52.927904 Epoch 268, Training loss 0.5520947824811082\n",
            "2022-03-29 04:08:09.255743 Epoch 269, Training loss 0.5510141109795217\n",
            "2022-03-29 04:08:25.553511 Epoch 270, Training loss 0.5500095382980679\n",
            "2022-03-29 04:08:41.786300 Epoch 271, Training loss 0.5495821528727441\n",
            "2022-03-29 04:08:58.075327 Epoch 272, Training loss 0.551873640636044\n",
            "2022-03-29 04:09:14.359201 Epoch 273, Training loss 0.5505801034743524\n",
            "2022-03-29 04:09:30.615193 Epoch 274, Training loss 0.5503251237408889\n",
            "2022-03-29 04:09:46.839757 Epoch 275, Training loss 0.5486739711535861\n",
            "2022-03-29 04:10:03.033443 Epoch 276, Training loss 0.5490220770659044\n",
            "2022-03-29 04:10:19.251615 Epoch 277, Training loss 0.5494598450944247\n",
            "2022-03-29 04:10:35.440554 Epoch 278, Training loss 0.5473735247121747\n",
            "2022-03-29 04:10:51.664006 Epoch 279, Training loss 0.5473000758977802\n",
            "2022-03-29 04:11:07.831234 Epoch 280, Training loss 0.5482738291287361\n",
            "2022-03-29 04:11:24.008225 Epoch 281, Training loss 0.5453469856544528\n",
            "2022-03-29 04:11:40.282370 Epoch 282, Training loss 0.5445881673061025\n",
            "2022-03-29 04:11:56.449175 Epoch 283, Training loss 0.5465167615648425\n",
            "2022-03-29 04:12:12.567345 Epoch 284, Training loss 0.5442030627251891\n",
            "2022-03-29 04:12:28.703909 Epoch 285, Training loss 0.5437659494331121\n",
            "2022-03-29 04:12:44.917162 Epoch 286, Training loss 0.5444902880950961\n",
            "2022-03-29 04:13:01.187396 Epoch 287, Training loss 0.5440796395320722\n",
            "2022-03-29 04:13:17.472282 Epoch 288, Training loss 0.5430201316809715\n",
            "2022-03-29 04:13:33.619026 Epoch 289, Training loss 0.541213220952417\n",
            "2022-03-29 04:13:49.892348 Epoch 290, Training loss 0.5417560293241535\n",
            "2022-03-29 04:14:06.166228 Epoch 291, Training loss 0.5411269783097155\n",
            "2022-03-29 04:14:22.302557 Epoch 292, Training loss 0.539753924969517\n",
            "2022-03-29 04:14:38.433490 Epoch 293, Training loss 0.5407832288147544\n",
            "2022-03-29 04:14:54.629069 Epoch 294, Training loss 0.5403732714598136\n",
            "2022-03-29 04:15:10.771279 Epoch 295, Training loss 0.5401983678417133\n",
            "2022-03-29 04:15:27.033748 Epoch 296, Training loss 0.5401988684978631\n",
            "2022-03-29 04:15:43.261740 Epoch 297, Training loss 0.5371984465004843\n",
            "2022-03-29 04:15:59.394683 Epoch 298, Training loss 0.53649991882198\n",
            "2022-03-29 04:16:15.584403 Epoch 299, Training loss 0.5378842502451309\n",
            "2022-03-29 04:16:31.724349 Epoch 300, Training loss 0.5379038864122633\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net().to(device=device)  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8_AkhNX5qBt",
        "outputId": "d4382751-662f-4a04-cd98-40d4e849805b"
      },
      "id": "H8_AkhNX5qBt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.79\n",
            "Accuracy val: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.b. Extend your network with two more additional hidden layers, like the example we did in lecture. Train your network for 300 epochs. Report your training time, loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare your model size and accuracy over the baseline implementation in Problem1. a. Do you see any over-fitting? Make sure to submit your code by providing the GitHub URL of your course repository for this course. (30pt)**"
      ],
      "metadata": {
        "id": "cxOq5ta7f82n"
      },
      "id": "cxOq5ta7f82n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9476994",
      "metadata": {
        "id": "b9476994"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(8, 3, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                        shuffle=True)\n",
        "model = Net().to(device=device)  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtL98diwo6te",
        "outputId": "0e7d4f1b-270e-4199-a097-77878bef0046"
      },
      "id": "VtL98diwo6te",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-29 04:44:05.192207 Epoch 1, Training loss 2.0476194139941573\n",
            "2022-03-29 04:44:21.416536 Epoch 2, Training loss 1.7932819879573325\n",
            "2022-03-29 04:44:37.600597 Epoch 3, Training loss 1.6093341150247227\n",
            "2022-03-29 04:44:53.748188 Epoch 4, Training loss 1.504915327519712\n",
            "2022-03-29 04:45:09.785505 Epoch 5, Training loss 1.4332176494171551\n",
            "2022-03-29 04:45:25.983206 Epoch 6, Training loss 1.3671491766524742\n",
            "2022-03-29 04:45:41.928765 Epoch 7, Training loss 1.310898762987093\n",
            "2022-03-29 04:45:58.000601 Epoch 8, Training loss 1.262955849128001\n",
            "2022-03-29 04:46:14.110734 Epoch 9, Training loss 1.2249980403486724\n",
            "2022-03-29 04:46:30.077334 Epoch 10, Training loss 1.1931186087448578\n",
            "2022-03-29 04:46:46.138664 Epoch 11, Training loss 1.1658514782290934\n",
            "2022-03-29 04:47:02.169020 Epoch 12, Training loss 1.1414087233336077\n",
            "2022-03-29 04:47:18.183312 Epoch 13, Training loss 1.1186294909328451\n",
            "2022-03-29 04:47:34.181483 Epoch 14, Training loss 1.1012593538255033\n",
            "2022-03-29 04:47:50.238129 Epoch 15, Training loss 1.0868839213762747\n",
            "2022-03-29 04:48:06.337656 Epoch 16, Training loss 1.0701049291111928\n",
            "2022-03-29 04:48:22.522027 Epoch 17, Training loss 1.056854472745715\n",
            "2022-03-29 04:48:38.616286 Epoch 18, Training loss 1.0434525705816802\n",
            "2022-03-29 04:48:54.695818 Epoch 19, Training loss 1.0318034239437268\n",
            "2022-03-29 04:49:10.536841 Epoch 20, Training loss 1.0214847234813758\n",
            "2022-03-29 04:49:26.704349 Epoch 21, Training loss 1.0116219730175975\n",
            "2022-03-29 04:49:42.837469 Epoch 22, Training loss 1.0008359894423229\n",
            "2022-03-29 04:49:59.143774 Epoch 23, Training loss 0.9899806528140211\n",
            "2022-03-29 04:50:15.395515 Epoch 24, Training loss 0.983061324528721\n",
            "2022-03-29 04:50:31.467212 Epoch 25, Training loss 0.9753647975604552\n",
            "2022-03-29 04:50:47.616223 Epoch 26, Training loss 0.9652918141211391\n",
            "2022-03-29 04:51:03.793097 Epoch 27, Training loss 0.9572945996318631\n",
            "2022-03-29 04:51:19.913082 Epoch 28, Training loss 0.9496668057368539\n",
            "2022-03-29 04:51:36.168776 Epoch 29, Training loss 0.9435373862533618\n",
            "2022-03-29 04:51:52.424330 Epoch 30, Training loss 0.9379475910187988\n",
            "2022-03-29 04:52:08.480048 Epoch 31, Training loss 0.9292231115233868\n",
            "2022-03-29 04:52:24.584962 Epoch 32, Training loss 0.9227231974187105\n",
            "2022-03-29 04:52:40.716858 Epoch 33, Training loss 0.9179274650943249\n",
            "2022-03-29 04:52:56.813626 Epoch 34, Training loss 0.9113619358795683\n",
            "2022-03-29 04:53:12.841020 Epoch 35, Training loss 0.9053229826795476\n",
            "2022-03-29 04:53:28.915168 Epoch 36, Training loss 0.8979318544382939\n",
            "2022-03-29 04:53:45.009522 Epoch 37, Training loss 0.8927240303867613\n",
            "2022-03-29 04:54:01.120619 Epoch 38, Training loss 0.8870264258226166\n",
            "2022-03-29 04:54:17.348029 Epoch 39, Training loss 0.8825809098113223\n",
            "2022-03-29 04:54:33.453362 Epoch 40, Training loss 0.8777755876178936\n",
            "2022-03-29 04:54:49.608349 Epoch 41, Training loss 0.8725566612104015\n",
            "2022-03-29 04:55:05.648131 Epoch 42, Training loss 0.8694420047580739\n",
            "2022-03-29 04:55:21.677865 Epoch 43, Training loss 0.8627901143201476\n",
            "2022-03-29 04:55:37.576346 Epoch 44, Training loss 0.8583850465772097\n",
            "2022-03-29 04:55:53.654063 Epoch 45, Training loss 0.8550797089591355\n",
            "2022-03-29 04:56:09.656316 Epoch 46, Training loss 0.8503186014835792\n",
            "2022-03-29 04:56:25.815512 Epoch 47, Training loss 0.8456568033493999\n",
            "2022-03-29 04:56:42.247932 Epoch 48, Training loss 0.8401629247552599\n",
            "2022-03-29 04:56:58.715357 Epoch 49, Training loss 0.8372280460489375\n",
            "2022-03-29 04:57:15.110087 Epoch 50, Training loss 0.8336238739892955\n",
            "2022-03-29 04:57:31.316966 Epoch 51, Training loss 0.8293508624710391\n",
            "2022-03-29 04:57:47.436771 Epoch 52, Training loss 0.8264960185874759\n",
            "2022-03-29 04:58:03.560021 Epoch 53, Training loss 0.8213632603740448\n",
            "2022-03-29 04:58:19.741156 Epoch 54, Training loss 0.8182226753676943\n",
            "2022-03-29 04:58:35.759712 Epoch 55, Training loss 0.8164703217918611\n",
            "2022-03-29 04:58:51.725288 Epoch 56, Training loss 0.813756138086319\n",
            "2022-03-29 04:59:07.667156 Epoch 57, Training loss 0.8101419415848944\n",
            "2022-03-29 04:59:23.743279 Epoch 58, Training loss 0.8061054366476396\n",
            "2022-03-29 04:59:39.870130 Epoch 59, Training loss 0.8010984800203377\n",
            "2022-03-29 04:59:56.045550 Epoch 60, Training loss 0.7984991505399079\n",
            "2022-03-29 05:00:12.214356 Epoch 61, Training loss 0.7957620618059812\n",
            "2022-03-29 05:00:28.442052 Epoch 62, Training loss 0.7933025333811256\n",
            "2022-03-29 05:00:44.664034 Epoch 63, Training loss 0.7895444822890679\n",
            "2022-03-29 05:01:00.785191 Epoch 64, Training loss 0.7865593171180667\n",
            "2022-03-29 05:01:16.990419 Epoch 65, Training loss 0.7829491484653005\n",
            "2022-03-29 05:01:33.413290 Epoch 66, Training loss 0.7814795731202416\n",
            "2022-03-29 05:01:49.731702 Epoch 67, Training loss 0.7781799213431985\n",
            "2022-03-29 05:02:06.157976 Epoch 68, Training loss 0.774701689972597\n",
            "2022-03-29 05:02:22.423843 Epoch 69, Training loss 0.7735352105725452\n",
            "2022-03-29 05:02:38.794065 Epoch 70, Training loss 0.7709909326127727\n",
            "2022-03-29 05:02:55.147945 Epoch 71, Training loss 0.7671134419300977\n",
            "2022-03-29 05:03:11.499643 Epoch 72, Training loss 0.7651188479131444\n",
            "2022-03-29 05:03:27.855662 Epoch 73, Training loss 0.7624215733669603\n",
            "2022-03-29 05:03:44.222196 Epoch 74, Training loss 0.7620118894159337\n",
            "2022-03-29 05:04:00.500132 Epoch 75, Training loss 0.758224579608044\n",
            "2022-03-29 05:04:16.985626 Epoch 76, Training loss 0.7559856083005896\n",
            "2022-03-29 05:04:33.470785 Epoch 77, Training loss 0.7530599034503292\n",
            "2022-03-29 05:04:49.796572 Epoch 78, Training loss 0.7514124558785992\n",
            "2022-03-29 05:05:06.192316 Epoch 79, Training loss 0.7475944198001071\n",
            "2022-03-29 05:05:22.538414 Epoch 80, Training loss 0.7468442001077525\n",
            "2022-03-29 05:05:38.848238 Epoch 81, Training loss 0.7458301747165372\n",
            "2022-03-29 05:05:55.187759 Epoch 82, Training loss 0.7420474498168282\n",
            "2022-03-29 05:06:11.521995 Epoch 83, Training loss 0.7395091111702687\n",
            "2022-03-29 05:06:27.740027 Epoch 84, Training loss 0.7389296510868975\n",
            "2022-03-29 05:06:44.020594 Epoch 85, Training loss 0.7363012990225917\n",
            "2022-03-29 05:07:00.295477 Epoch 86, Training loss 0.7337375617850467\n",
            "2022-03-29 05:07:16.686984 Epoch 87, Training loss 0.7311524321203646\n",
            "2022-03-29 05:07:33.061906 Epoch 88, Training loss 0.7298841092668836\n",
            "2022-03-29 05:07:49.451343 Epoch 89, Training loss 0.7256599239376195\n",
            "2022-03-29 05:08:05.828317 Epoch 90, Training loss 0.7261931668309605\n",
            "2022-03-29 05:08:22.186371 Epoch 91, Training loss 0.7236102866699629\n",
            "2022-03-29 05:08:38.367131 Epoch 92, Training loss 0.722677390898585\n",
            "2022-03-29 05:08:54.456804 Epoch 93, Training loss 0.7207249682730116\n",
            "2022-03-29 05:09:10.619273 Epoch 94, Training loss 0.7184768036350875\n",
            "2022-03-29 05:09:26.924395 Epoch 95, Training loss 0.7165889101641257\n",
            "2022-03-29 05:09:43.060503 Epoch 96, Training loss 0.7152631839218042\n",
            "2022-03-29 05:09:59.216285 Epoch 97, Training loss 0.7126118961883627\n",
            "2022-03-29 05:10:15.469308 Epoch 98, Training loss 0.7101365947891074\n",
            "2022-03-29 05:10:31.759729 Epoch 99, Training loss 0.7079432414239629\n",
            "2022-03-29 05:10:48.063360 Epoch 100, Training loss 0.7068493322795614\n",
            "2022-03-29 05:11:04.418476 Epoch 101, Training loss 0.7042073158885512\n",
            "2022-03-29 05:11:20.856285 Epoch 102, Training loss 0.7033774850847166\n",
            "2022-03-29 05:11:37.208417 Epoch 103, Training loss 0.7010753852174715\n",
            "2022-03-29 05:11:53.521291 Epoch 104, Training loss 0.7004518805409942\n",
            "2022-03-29 05:12:09.939656 Epoch 105, Training loss 0.6983554868213356\n",
            "2022-03-29 05:12:26.340926 Epoch 106, Training loss 0.6975895401157076\n",
            "2022-03-29 05:12:42.832253 Epoch 107, Training loss 0.6949667753008626\n",
            "2022-03-29 05:12:59.269283 Epoch 108, Training loss 0.6944683361083955\n",
            "2022-03-29 05:13:15.746763 Epoch 109, Training loss 0.6921649193748489\n",
            "2022-03-29 05:13:32.203806 Epoch 110, Training loss 0.6913974325522743\n",
            "2022-03-29 05:13:48.619939 Epoch 111, Training loss 0.6892339372269028\n",
            "2022-03-29 05:14:04.971653 Epoch 112, Training loss 0.6873663065149961\n",
            "2022-03-29 05:14:21.389775 Epoch 113, Training loss 0.6857754708555959\n",
            "2022-03-29 05:14:37.712599 Epoch 114, Training loss 0.6843395745738998\n",
            "2022-03-29 05:14:54.050575 Epoch 115, Training loss 0.6821757610267996\n",
            "2022-03-29 05:15:10.366049 Epoch 116, Training loss 0.6809194871150624\n",
            "2022-03-29 05:15:26.686725 Epoch 117, Training loss 0.6802813976698214\n",
            "2022-03-29 05:15:42.966501 Epoch 118, Training loss 0.6793984975427618\n",
            "2022-03-29 05:15:59.296655 Epoch 119, Training loss 0.6775040169201239\n",
            "2022-03-29 05:16:15.471908 Epoch 120, Training loss 0.674044982780276\n",
            "2022-03-29 05:16:31.517647 Epoch 121, Training loss 0.6749519488924299\n",
            "2022-03-29 05:16:47.606049 Epoch 122, Training loss 0.6733487541108485\n",
            "2022-03-29 05:17:03.647727 Epoch 123, Training loss 0.671272263121422\n",
            "2022-03-29 05:17:19.702097 Epoch 124, Training loss 0.6704850475044202\n",
            "2022-03-29 05:17:35.853652 Epoch 125, Training loss 0.6690996259527133\n",
            "2022-03-29 05:17:52.021799 Epoch 126, Training loss 0.6680172080617122\n",
            "2022-03-29 05:18:08.037349 Epoch 127, Training loss 0.6677329477751651\n",
            "2022-03-29 05:18:24.190863 Epoch 128, Training loss 0.665081941258267\n",
            "2022-03-29 05:18:40.253606 Epoch 129, Training loss 0.6628265112181149\n",
            "2022-03-29 05:18:56.426214 Epoch 130, Training loss 0.661802287411202\n",
            "2022-03-29 05:19:12.456304 Epoch 131, Training loss 0.6612380690053296\n",
            "2022-03-29 05:19:28.446199 Epoch 132, Training loss 0.6594282483200893\n",
            "2022-03-29 05:19:44.456573 Epoch 133, Training loss 0.6575920630598922\n",
            "2022-03-29 05:20:00.410742 Epoch 134, Training loss 0.6563628097171978\n",
            "2022-03-29 05:20:16.528157 Epoch 135, Training loss 0.6555610448884233\n",
            "2022-03-29 05:20:32.657211 Epoch 136, Training loss 0.6530215984110332\n",
            "2022-03-29 05:20:48.735671 Epoch 137, Training loss 0.6545678715571723\n",
            "2022-03-29 05:21:04.839201 Epoch 138, Training loss 0.6511830487824461\n",
            "2022-03-29 05:21:20.902755 Epoch 139, Training loss 0.6506876624987253\n",
            "2022-03-29 05:21:36.937062 Epoch 140, Training loss 0.6511643521316216\n",
            "2022-03-29 05:21:53.108963 Epoch 141, Training loss 0.6479902057848927\n",
            "2022-03-29 05:22:09.214874 Epoch 142, Training loss 0.6464533220662181\n",
            "2022-03-29 05:22:25.455116 Epoch 143, Training loss 0.6460484148520033\n",
            "2022-03-29 05:22:41.705958 Epoch 144, Training loss 0.6438361210057802\n",
            "2022-03-29 05:22:57.795727 Epoch 145, Training loss 0.6438819955453239\n",
            "2022-03-29 05:23:13.870513 Epoch 146, Training loss 0.6428414567199814\n",
            "2022-03-29 05:23:29.983650 Epoch 147, Training loss 0.6409613485912533\n",
            "2022-03-29 05:23:46.161258 Epoch 148, Training loss 0.6416370103045193\n",
            "2022-03-29 05:24:02.229203 Epoch 149, Training loss 0.639247388257395\n",
            "2022-03-29 05:24:18.287381 Epoch 150, Training loss 0.6376032215135786\n",
            "2022-03-29 05:24:34.379023 Epoch 151, Training loss 0.6372024007236866\n",
            "2022-03-29 05:24:50.485346 Epoch 152, Training loss 0.6348268769281294\n",
            "2022-03-29 05:25:06.497002 Epoch 153, Training loss 0.6343780055908901\n",
            "2022-03-29 05:25:22.687432 Epoch 154, Training loss 0.6337571932607905\n",
            "2022-03-29 05:25:38.702976 Epoch 155, Training loss 0.6308617931802559\n",
            "2022-03-29 05:25:54.814919 Epoch 156, Training loss 0.6329276029716062\n",
            "2022-03-29 05:26:10.959511 Epoch 157, Training loss 0.630887910609355\n",
            "2022-03-29 05:26:27.117345 Epoch 158, Training loss 0.6293063723217801\n",
            "2022-03-29 05:26:43.141965 Epoch 159, Training loss 0.6294451465692057\n",
            "2022-03-29 05:26:59.157347 Epoch 160, Training loss 0.626173249767412\n",
            "2022-03-29 05:27:15.255347 Epoch 161, Training loss 0.6252308866328291\n",
            "2022-03-29 05:27:31.187492 Epoch 162, Training loss 0.6256789554034352\n",
            "2022-03-29 05:27:47.320684 Epoch 163, Training loss 0.6237681304554805\n",
            "2022-03-29 05:28:03.377052 Epoch 164, Training loss 0.622890898829226\n",
            "2022-03-29 05:28:19.499390 Epoch 165, Training loss 0.6215645253963178\n",
            "2022-03-29 05:28:35.531413 Epoch 166, Training loss 0.6214041505628229\n",
            "2022-03-29 05:28:51.787685 Epoch 167, Training loss 0.6207436998100841\n",
            "2022-03-29 05:29:08.101760 Epoch 168, Training loss 0.6188998521898713\n",
            "2022-03-29 05:29:24.514632 Epoch 169, Training loss 0.6184129768320362\n",
            "2022-03-29 05:29:40.866583 Epoch 170, Training loss 0.6176199038772632\n",
            "2022-03-29 05:29:57.114353 Epoch 171, Training loss 0.6175713888214677\n",
            "2022-03-29 05:30:13.146666 Epoch 172, Training loss 0.6166385512446504\n",
            "2022-03-29 05:30:29.234584 Epoch 173, Training loss 0.6158214133337635\n",
            "2022-03-29 05:30:45.418376 Epoch 174, Training loss 0.6133546421823599\n",
            "2022-03-29 05:31:01.494542 Epoch 175, Training loss 0.6126716159798605\n",
            "2022-03-29 05:31:17.516209 Epoch 176, Training loss 0.6112935559447769\n",
            "2022-03-29 05:31:33.610674 Epoch 177, Training loss 0.6096830413774457\n",
            "2022-03-29 05:31:49.739009 Epoch 178, Training loss 0.6101785112372444\n",
            "2022-03-29 05:32:05.787317 Epoch 179, Training loss 0.6093615380394489\n",
            "2022-03-29 05:32:21.927724 Epoch 180, Training loss 0.6093639831637483\n",
            "2022-03-29 05:32:38.068341 Epoch 181, Training loss 0.6085558655621756\n",
            "2022-03-29 05:32:54.189526 Epoch 182, Training loss 0.6040799456560398\n",
            "2022-03-29 05:33:10.332434 Epoch 183, Training loss 0.6052494557464824\n",
            "2022-03-29 05:33:26.530624 Epoch 184, Training loss 0.6057535899264733\n",
            "2022-03-29 05:33:42.485823 Epoch 185, Training loss 0.6033020370146808\n",
            "2022-03-29 05:33:58.563049 Epoch 186, Training loss 0.6031048941185407\n",
            "2022-03-29 05:34:14.515393 Epoch 187, Training loss 0.601732166869866\n",
            "2022-03-29 05:34:30.620482 Epoch 188, Training loss 0.601836519358713\n",
            "2022-03-29 05:34:46.659798 Epoch 189, Training loss 0.59973214802992\n",
            "2022-03-29 05:35:02.928655 Epoch 190, Training loss 0.6000208354667019\n",
            "2022-03-29 05:35:19.188494 Epoch 191, Training loss 0.5999509330524508\n",
            "2022-03-29 05:35:35.436675 Epoch 192, Training loss 0.5991985428211329\n",
            "2022-03-29 05:35:51.639259 Epoch 193, Training loss 0.5971212307434253\n",
            "2022-03-29 05:36:07.706111 Epoch 194, Training loss 0.5994096344617932\n",
            "2022-03-29 05:36:23.743776 Epoch 195, Training loss 0.5950735055881998\n",
            "2022-03-29 05:36:39.823846 Epoch 196, Training loss 0.5961785111433405\n",
            "2022-03-29 05:36:55.819530 Epoch 197, Training loss 0.5939326539368885\n",
            "2022-03-29 05:37:11.845541 Epoch 198, Training loss 0.5926487493469282\n",
            "2022-03-29 05:37:28.086380 Epoch 199, Training loss 0.5929422341572964\n",
            "2022-03-29 05:37:44.106796 Epoch 200, Training loss 0.5931231443915526\n",
            "2022-03-29 05:38:00.113996 Epoch 201, Training loss 0.5915830689470482\n",
            "2022-03-29 05:38:16.135141 Epoch 202, Training loss 0.5925029303754688\n",
            "2022-03-29 05:38:32.133376 Epoch 203, Training loss 0.5900381991015676\n",
            "2022-03-29 05:38:48.213617 Epoch 204, Training loss 0.5896815969358624\n",
            "2022-03-29 05:39:04.311751 Epoch 205, Training loss 0.5893683549769394\n",
            "2022-03-29 05:39:20.207990 Epoch 206, Training loss 0.5876794257356078\n",
            "2022-03-29 05:39:36.216644 Epoch 207, Training loss 0.5883955053813622\n",
            "2022-03-29 05:39:52.405756 Epoch 208, Training loss 0.5880877391990188\n",
            "2022-03-29 05:40:08.634349 Epoch 209, Training loss 0.5850999141516893\n",
            "2022-03-29 05:40:24.804732 Epoch 210, Training loss 0.5843361791061319\n",
            "2022-03-29 05:40:40.982028 Epoch 211, Training loss 0.5852850006364495\n",
            "2022-03-29 05:40:57.196278 Epoch 212, Training loss 0.5833929780765873\n",
            "2022-03-29 05:41:13.479314 Epoch 213, Training loss 0.5825844594203603\n",
            "2022-03-29 05:41:29.743034 Epoch 214, Training loss 0.5820154455845313\n",
            "2022-03-29 05:41:45.993389 Epoch 215, Training loss 0.5822297081236949\n",
            "2022-03-29 05:42:02.233400 Epoch 216, Training loss 0.5809567806970738\n",
            "2022-03-29 05:42:18.338782 Epoch 217, Training loss 0.5802055150270462\n",
            "2022-03-29 05:42:34.399547 Epoch 218, Training loss 0.5793245681716354\n",
            "2022-03-29 05:42:50.435410 Epoch 219, Training loss 0.5788457201950995\n",
            "2022-03-29 05:43:06.544666 Epoch 220, Training loss 0.5793919356354057\n",
            "2022-03-29 05:43:22.653640 Epoch 221, Training loss 0.5777945587854556\n",
            "2022-03-29 05:43:38.720293 Epoch 222, Training loss 0.5760615874281929\n",
            "2022-03-29 05:43:54.842643 Epoch 223, Training loss 0.5773381447167043\n",
            "2022-03-29 05:44:10.925067 Epoch 224, Training loss 0.5762601769565011\n",
            "2022-03-29 05:44:26.946253 Epoch 225, Training loss 0.5771259006560611\n",
            "2022-03-29 05:44:43.032335 Epoch 226, Training loss 0.574509388764801\n",
            "2022-03-29 05:44:59.197569 Epoch 227, Training loss 0.5761210400887462\n",
            "2022-03-29 05:45:15.510501 Epoch 228, Training loss 0.573408106365777\n",
            "2022-03-29 05:45:31.888448 Epoch 229, Training loss 0.5739665261619841\n",
            "2022-03-29 05:45:48.213769 Epoch 230, Training loss 0.5732763382556189\n",
            "2022-03-29 05:46:04.604953 Epoch 231, Training loss 0.571650506056788\n",
            "2022-03-29 05:46:20.845319 Epoch 232, Training loss 0.5730152896336277\n",
            "2022-03-29 05:46:37.115541 Epoch 233, Training loss 0.5704127281828\n",
            "2022-03-29 05:46:53.172184 Epoch 234, Training loss 0.5702229269096614\n",
            "2022-03-29 05:47:09.406281 Epoch 235, Training loss 0.5684887075515659\n",
            "2022-03-29 05:47:25.684814 Epoch 236, Training loss 0.5682201350436491\n",
            "2022-03-29 05:47:41.955706 Epoch 237, Training loss 0.5684937039375915\n",
            "2022-03-29 05:47:58.215829 Epoch 238, Training loss 0.5682620364229393\n",
            "2022-03-29 05:48:14.318140 Epoch 239, Training loss 0.5646616827953806\n",
            "2022-03-29 05:48:30.490021 Epoch 240, Training loss 0.5663297228572314\n",
            "2022-03-29 05:48:46.683819 Epoch 241, Training loss 0.5669150487388797\n",
            "2022-03-29 05:49:02.935714 Epoch 242, Training loss 0.5647468808895487\n",
            "2022-03-29 05:49:19.202445 Epoch 243, Training loss 0.564939545014935\n",
            "2022-03-29 05:49:35.274191 Epoch 244, Training loss 0.5638445688940376\n",
            "2022-03-29 05:49:51.452371 Epoch 245, Training loss 0.5623786911139708\n",
            "2022-03-29 05:50:07.693791 Epoch 246, Training loss 0.5635920532447908\n",
            "2022-03-29 05:50:23.894362 Epoch 247, Training loss 0.5614409918904\n",
            "2022-03-29 05:50:40.042547 Epoch 248, Training loss 0.5628163662103131\n",
            "2022-03-29 05:50:56.270880 Epoch 249, Training loss 0.5607632660423704\n",
            "2022-03-29 05:51:12.537540 Epoch 250, Training loss 0.561782169734578\n",
            "2022-03-29 05:51:28.877567 Epoch 251, Training loss 0.5597875880844453\n",
            "2022-03-29 05:51:45.117232 Epoch 252, Training loss 0.559905415110271\n",
            "2022-03-29 05:52:01.277122 Epoch 253, Training loss 0.5597169877737379\n",
            "2022-03-29 05:52:17.455724 Epoch 254, Training loss 0.5582519879807597\n",
            "2022-03-29 05:52:33.685264 Epoch 255, Training loss 0.5581415732345922\n",
            "2022-03-29 05:52:49.855193 Epoch 256, Training loss 0.5568709260667376\n",
            "2022-03-29 05:53:05.975169 Epoch 257, Training loss 0.5573793476271203\n",
            "2022-03-29 05:53:22.056290 Epoch 258, Training loss 0.5588511617287345\n",
            "2022-03-29 05:53:38.077305 Epoch 259, Training loss 0.5561833486456396\n",
            "2022-03-29 05:53:54.263310 Epoch 260, Training loss 0.5568616594881048\n",
            "2022-03-29 05:54:10.361367 Epoch 261, Training loss 0.5541683801299776\n",
            "2022-03-29 05:54:26.548321 Epoch 262, Training loss 0.5554663963101404\n",
            "2022-03-29 05:54:42.774011 Epoch 263, Training loss 0.5567390577643728\n",
            "2022-03-29 05:54:58.993755 Epoch 264, Training loss 0.552203243383971\n",
            "2022-03-29 05:55:15.174742 Epoch 265, Training loss 0.5535541249967902\n",
            "2022-03-29 05:55:31.299695 Epoch 266, Training loss 0.5533432598271029\n",
            "2022-03-29 05:55:47.486214 Epoch 267, Training loss 0.5528468256411345\n",
            "2022-03-29 05:56:03.484897 Epoch 268, Training loss 0.551628080989851\n",
            "2022-03-29 05:56:19.605460 Epoch 269, Training loss 0.5517893302471132\n",
            "2022-03-29 05:56:35.745257 Epoch 270, Training loss 0.5508224909072337\n",
            "2022-03-29 05:56:51.928130 Epoch 271, Training loss 0.550318009046185\n",
            "2022-03-29 05:57:08.052271 Epoch 272, Training loss 0.549383996316539\n",
            "2022-03-29 05:57:24.343829 Epoch 273, Training loss 0.5505934296666509\n",
            "2022-03-29 05:57:40.547338 Epoch 274, Training loss 0.5479320099058054\n",
            "2022-03-29 05:57:56.654853 Epoch 275, Training loss 0.5486197203702634\n",
            "2022-03-29 05:58:12.695719 Epoch 276, Training loss 0.5487419070718843\n",
            "2022-03-29 05:58:28.770874 Epoch 277, Training loss 0.5481776609597608\n",
            "2022-03-29 05:58:44.986519 Epoch 278, Training loss 0.5477987147505631\n",
            "2022-03-29 05:59:01.174360 Epoch 279, Training loss 0.5484573876156527\n",
            "2022-03-29 05:59:17.433993 Epoch 280, Training loss 0.5467811137285379\n",
            "2022-03-29 05:59:33.538126 Epoch 281, Training loss 0.5468302696867062\n",
            "2022-03-29 05:59:49.616379 Epoch 282, Training loss 0.5471786831689003\n",
            "2022-03-29 06:00:05.767420 Epoch 283, Training loss 0.544730999128288\n",
            "2022-03-29 06:00:21.902019 Epoch 284, Training loss 0.5428585512635044\n",
            "2022-03-29 06:00:38.023452 Epoch 285, Training loss 0.5427479487855721\n",
            "2022-03-29 06:00:54.413099 Epoch 286, Training loss 0.5445341820378438\n",
            "2022-03-29 06:01:10.846222 Epoch 287, Training loss 0.5450218313795221\n",
            "2022-03-29 06:01:27.284173 Epoch 288, Training loss 0.5419838310355116\n",
            "2022-03-29 06:01:43.692445 Epoch 289, Training loss 0.5409448249527561\n",
            "2022-03-29 06:02:00.135070 Epoch 290, Training loss 0.5410751961838559\n",
            "2022-03-29 06:02:16.534394 Epoch 291, Training loss 0.5388203115795579\n",
            "2022-03-29 06:02:32.890203 Epoch 292, Training loss 0.5426470129310018\n",
            "2022-03-29 06:02:49.289490 Epoch 293, Training loss 0.5402962298458799\n",
            "2022-03-29 06:03:05.617042 Epoch 294, Training loss 0.5409918176319898\n",
            "2022-03-29 06:03:21.854583 Epoch 295, Training loss 0.5388028282872246\n",
            "2022-03-29 06:03:38.040983 Epoch 296, Training loss 0.5398437237305105\n",
            "2022-03-29 06:03:54.287460 Epoch 297, Training loss 0.5371148009281939\n",
            "2022-03-29 06:04:10.494629 Epoch 298, Training loss 0.5383988131037758\n",
            "2022-03-29 06:04:26.734491 Epoch 299, Training loss 0.539555762201319\n",
            "2022-03-29 06:04:42.855158 Epoch 300, Training loss 0.5370135453274792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plHXrzAf-lIh",
        "outputId": "c3bc3e53-2bcb-4d82-ecb8-3abb6a3fb381"
      },
      "id": "plHXrzAf-lIh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.a. Build a Convolutional Neural Network, like what we built in lectures to classify the images across all 10 classes in CIFAR 10. You need to adjust the fully connected layer at the end properly with respect to the number of output classes. Train your network for 300 epochs. Report your training time, training loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare them against a fully connected network (Problem 1) on training time, achieved accuracy, and model size. Make sure to submit your code by providing the GitHub URL of your course repository for this course (20pt)"
      ],
      "metadata": {
        "id": "Qnh83r-sgFOm"
      },
      "id": "Qnh83r-sgFOm"
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "AfyMoNCOnpr5"
      },
      "id": "AfyMoNCOnpr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwuuv38Jwvd7",
        "outputId": "3b06f713-70ff-4fcb-d965-d5d3c0fd0592"
      },
      "id": "Uwuuv38Jwvd7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 14:56:48.653678 Epoch 1, Training loss 2.118773611915081\n",
            "2022-03-30 14:56:59.732190 Epoch 2, Training loss 1.7503244863141834\n",
            "2022-03-30 14:57:10.799833 Epoch 3, Training loss 1.5545217981728752\n",
            "2022-03-30 14:57:21.884237 Epoch 4, Training loss 1.4413532382996797\n",
            "2022-03-30 14:57:32.987184 Epoch 5, Training loss 1.3589966778864946\n",
            "2022-03-30 14:57:43.981476 Epoch 6, Training loss 1.2896170318126678\n",
            "2022-03-30 14:57:55.056163 Epoch 7, Training loss 1.2293713762022345\n",
            "2022-03-30 14:58:06.258686 Epoch 8, Training loss 1.180406075151985\n",
            "2022-03-30 14:58:17.535626 Epoch 9, Training loss 1.1376883311344839\n",
            "2022-03-30 14:58:28.579210 Epoch 10, Training loss 1.0984396485568921\n",
            "2022-03-30 14:58:39.589001 Epoch 11, Training loss 1.0637750482315298\n",
            "2022-03-30 14:58:50.588554 Epoch 12, Training loss 1.034006581205846\n",
            "2022-03-30 14:59:01.586133 Epoch 13, Training loss 1.00559051735017\n",
            "2022-03-30 14:59:12.436030 Epoch 14, Training loss 0.9804429912658603\n",
            "2022-03-30 14:59:23.493625 Epoch 15, Training loss 0.9606560496875393\n",
            "2022-03-30 14:59:34.460573 Epoch 16, Training loss 0.9414867086483695\n",
            "2022-03-30 14:59:45.386743 Epoch 17, Training loss 0.9202986186575097\n",
            "2022-03-30 14:59:56.301743 Epoch 18, Training loss 0.906030728765156\n",
            "2022-03-30 15:00:07.325005 Epoch 19, Training loss 0.8909446919513175\n",
            "2022-03-30 15:00:18.233233 Epoch 20, Training loss 0.8786013121037837\n",
            "2022-03-30 15:00:29.576311 Epoch 21, Training loss 0.8657008769262172\n",
            "2022-03-30 15:00:40.807120 Epoch 22, Training loss 0.8546449686865063\n",
            "2022-03-30 15:00:51.999426 Epoch 23, Training loss 0.8469206514718283\n",
            "2022-03-30 15:01:03.136457 Epoch 24, Training loss 0.83377184347271\n",
            "2022-03-30 15:01:14.353072 Epoch 25, Training loss 0.8233626543560906\n",
            "2022-03-30 15:01:25.477599 Epoch 26, Training loss 0.8129425452036017\n",
            "2022-03-30 15:01:36.680657 Epoch 27, Training loss 0.8074885737484373\n",
            "2022-03-30 15:01:47.855700 Epoch 28, Training loss 0.8019598277709673\n",
            "2022-03-30 15:01:59.190579 Epoch 29, Training loss 0.7942344972392177\n",
            "2022-03-30 15:02:10.265281 Epoch 30, Training loss 0.7817112526015553\n",
            "2022-03-30 15:02:21.344459 Epoch 31, Training loss 0.7764894690964838\n",
            "2022-03-30 15:02:32.554715 Epoch 32, Training loss 0.7712509962527648\n",
            "2022-03-30 15:02:43.697700 Epoch 33, Training loss 0.7665930837773911\n",
            "2022-03-30 15:02:54.736251 Epoch 34, Training loss 0.7599880527276213\n",
            "2022-03-30 15:03:06.061257 Epoch 35, Training loss 0.7539466165215768\n",
            "2022-03-30 15:03:17.093969 Epoch 36, Training loss 0.7494864399399599\n",
            "2022-03-30 15:03:28.257615 Epoch 37, Training loss 0.7437976384940355\n",
            "2022-03-30 15:03:39.371032 Epoch 38, Training loss 0.7400852395293048\n",
            "2022-03-30 15:03:50.415480 Epoch 39, Training loss 0.7323405106201806\n",
            "2022-03-30 15:04:01.350094 Epoch 40, Training loss 0.7275295770153061\n",
            "2022-03-30 15:04:12.416143 Epoch 41, Training loss 0.7233181361804533\n",
            "2022-03-30 15:04:23.433728 Epoch 42, Training loss 0.7209753567148047\n",
            "2022-03-30 15:04:34.624781 Epoch 43, Training loss 0.7133496946767163\n",
            "2022-03-30 15:04:45.862508 Epoch 44, Training loss 0.7090958898024791\n",
            "2022-03-30 15:04:56.930527 Epoch 45, Training loss 0.7072001125120446\n",
            "2022-03-30 15:05:09.196214 Epoch 46, Training loss 0.7008611272515544\n",
            "2022-03-30 15:05:20.318082 Epoch 47, Training loss 0.6984078295700386\n",
            "2022-03-30 15:05:31.353560 Epoch 48, Training loss 0.6952729395131017\n",
            "2022-03-30 15:05:42.474122 Epoch 49, Training loss 0.6891982193721835\n",
            "2022-03-30 15:05:53.409145 Epoch 50, Training loss 0.6850337057238649\n",
            "2022-03-30 15:06:04.315678 Epoch 51, Training loss 0.683263395753358\n",
            "2022-03-30 15:06:15.308884 Epoch 52, Training loss 0.6783122840287436\n",
            "2022-03-30 15:06:26.312892 Epoch 53, Training loss 0.6762096203882676\n",
            "2022-03-30 15:06:37.451548 Epoch 54, Training loss 0.6704352251861406\n",
            "2022-03-30 15:06:48.542838 Epoch 55, Training loss 0.6694576023408519\n",
            "2022-03-30 15:06:59.384434 Epoch 56, Training loss 0.6637631957716954\n",
            "2022-03-30 15:07:10.470068 Epoch 57, Training loss 0.6609728534508239\n",
            "2022-03-30 15:07:21.685284 Epoch 58, Training loss 0.6571571895914614\n",
            "2022-03-30 15:07:32.716202 Epoch 59, Training loss 0.6549206197338031\n",
            "2022-03-30 15:07:43.753298 Epoch 60, Training loss 0.6509967686803749\n",
            "2022-03-30 15:07:54.783990 Epoch 61, Training loss 0.6502219287254621\n",
            "2022-03-30 15:08:05.768174 Epoch 62, Training loss 0.6446007296176213\n",
            "2022-03-30 15:08:16.873612 Epoch 63, Training loss 0.6407300718605061\n",
            "2022-03-30 15:08:27.894933 Epoch 64, Training loss 0.6383016051919869\n",
            "2022-03-30 15:08:38.989679 Epoch 65, Training loss 0.6370272739692722\n",
            "2022-03-30 15:08:50.146091 Epoch 66, Training loss 0.6322703903822033\n",
            "2022-03-30 15:09:01.077391 Epoch 67, Training loss 0.6303918932557411\n",
            "2022-03-30 15:09:12.006553 Epoch 68, Training loss 0.626948209789098\n",
            "2022-03-30 15:09:22.878199 Epoch 69, Training loss 0.6271358498222078\n",
            "2022-03-30 15:09:33.720884 Epoch 70, Training loss 0.6228158662996024\n",
            "2022-03-30 15:09:44.727353 Epoch 71, Training loss 0.6175048470954456\n",
            "2022-03-30 15:09:55.581009 Epoch 72, Training loss 0.6166377479158094\n",
            "2022-03-30 15:10:06.410522 Epoch 73, Training loss 0.6153290785487046\n",
            "2022-03-30 15:10:17.318899 Epoch 74, Training loss 0.6090281739106873\n",
            "2022-03-30 15:10:28.259080 Epoch 75, Training loss 0.6097492148046908\n",
            "2022-03-30 15:10:39.226238 Epoch 76, Training loss 0.6062295513079904\n",
            "2022-03-30 15:10:50.134548 Epoch 77, Training loss 0.6061411221771289\n",
            "2022-03-30 15:11:00.939786 Epoch 78, Training loss 0.6038710573292754\n",
            "2022-03-30 15:11:11.792339 Epoch 79, Training loss 0.5991707745644138\n",
            "2022-03-30 15:11:22.637893 Epoch 80, Training loss 0.5978181346526841\n",
            "2022-03-30 15:11:33.569004 Epoch 81, Training loss 0.5947306451895048\n",
            "2022-03-30 15:11:44.443878 Epoch 82, Training loss 0.5926392013993105\n",
            "2022-03-30 15:11:55.414098 Epoch 83, Training loss 0.5906372314218975\n",
            "2022-03-30 15:12:06.418497 Epoch 84, Training loss 0.5868694292919715\n",
            "2022-03-30 15:12:17.285003 Epoch 85, Training loss 0.5826910506467076\n",
            "2022-03-30 15:12:28.129777 Epoch 86, Training loss 0.5825334746971764\n",
            "2022-03-30 15:12:39.080031 Epoch 87, Training loss 0.5799855024689604\n",
            "2022-03-30 15:12:50.286774 Epoch 88, Training loss 0.5801700477481193\n",
            "2022-03-30 15:13:01.220421 Epoch 89, Training loss 0.5767086303371298\n",
            "2022-03-30 15:13:12.136970 Epoch 90, Training loss 0.5768162899691126\n",
            "2022-03-30 15:13:23.146245 Epoch 91, Training loss 0.5734181473855777\n",
            "2022-03-30 15:13:34.038456 Epoch 92, Training loss 0.5696832796992244\n",
            "2022-03-30 15:13:45.002477 Epoch 93, Training loss 0.5666767947585382\n",
            "2022-03-30 15:13:55.942469 Epoch 94, Training loss 0.5690761014933476\n",
            "2022-03-30 15:14:06.937755 Epoch 95, Training loss 0.5665140563188611\n",
            "2022-03-30 15:14:17.804669 Epoch 96, Training loss 0.5605231926150029\n",
            "2022-03-30 15:14:28.706450 Epoch 97, Training loss 0.5623006741790211\n",
            "2022-03-30 15:14:39.722316 Epoch 98, Training loss 0.5586738145869711\n",
            "2022-03-30 15:14:50.672315 Epoch 99, Training loss 0.5563509542390209\n",
            "2022-03-30 15:15:01.529433 Epoch 100, Training loss 0.5547352012465981\n",
            "2022-03-30 15:15:12.407329 Epoch 101, Training loss 0.5567993900126509\n",
            "2022-03-30 15:15:23.352286 Epoch 102, Training loss 0.5532470455826701\n",
            "2022-03-30 15:15:34.229757 Epoch 103, Training loss 0.5505331079368396\n",
            "2022-03-30 15:15:45.109187 Epoch 104, Training loss 0.5492302125219799\n",
            "2022-03-30 15:15:55.929113 Epoch 105, Training loss 0.5468060968972533\n",
            "2022-03-30 15:16:06.776664 Epoch 106, Training loss 0.5459140317747965\n",
            "2022-03-30 15:16:17.625458 Epoch 107, Training loss 0.5453130624559529\n",
            "2022-03-30 15:16:28.406476 Epoch 108, Training loss 0.543751213823438\n",
            "2022-03-30 15:16:39.274975 Epoch 109, Training loss 0.5420519996177205\n",
            "2022-03-30 15:16:50.259897 Epoch 110, Training loss 0.5388397763071158\n",
            "2022-03-30 15:17:01.236701 Epoch 111, Training loss 0.5380001382335372\n",
            "2022-03-30 15:17:11.992770 Epoch 112, Training loss 0.5353419974141413\n",
            "2022-03-30 15:17:22.880520 Epoch 113, Training loss 0.5365340115545351\n",
            "2022-03-30 15:17:33.726874 Epoch 114, Training loss 0.534332173578727\n",
            "2022-03-30 15:17:44.604969 Epoch 115, Training loss 0.5306421275181539\n",
            "2022-03-30 15:17:55.644203 Epoch 116, Training loss 0.5306580975994735\n",
            "2022-03-30 15:18:06.729855 Epoch 117, Training loss 0.5297476486743563\n",
            "2022-03-30 15:18:17.677868 Epoch 118, Training loss 0.526362227928608\n",
            "2022-03-30 15:18:28.671279 Epoch 119, Training loss 0.525345313770082\n",
            "2022-03-30 15:18:39.524044 Epoch 120, Training loss 0.52429675394693\n",
            "2022-03-30 15:18:50.647322 Epoch 121, Training loss 0.5250685135155078\n",
            "2022-03-30 15:19:01.490875 Epoch 122, Training loss 0.5209017728105225\n",
            "2022-03-30 15:19:12.321242 Epoch 123, Training loss 0.5205220824388592\n",
            "2022-03-30 15:19:23.226531 Epoch 124, Training loss 0.5174020377685652\n",
            "2022-03-30 15:19:34.057823 Epoch 125, Training loss 0.5184927915825563\n",
            "2022-03-30 15:19:44.921720 Epoch 126, Training loss 0.5156420591999503\n",
            "2022-03-30 15:19:55.822156 Epoch 127, Training loss 0.5145645917814863\n",
            "2022-03-30 15:20:06.755285 Epoch 128, Training loss 0.5118953644695794\n",
            "2022-03-30 15:20:17.678954 Epoch 129, Training loss 0.514681896895094\n",
            "2022-03-30 15:20:28.455404 Epoch 130, Training loss 0.5110891578561815\n",
            "2022-03-30 15:20:39.352950 Epoch 131, Training loss 0.5115310431593825\n",
            "2022-03-30 15:20:50.216916 Epoch 132, Training loss 0.5075635847723697\n",
            "2022-03-30 15:21:01.159231 Epoch 133, Training loss 0.5057346445062886\n",
            "2022-03-30 15:21:12.034735 Epoch 134, Training loss 0.5074940892817724\n",
            "2022-03-30 15:21:23.085064 Epoch 135, Training loss 0.5056312002070115\n",
            "2022-03-30 15:21:33.933191 Epoch 136, Training loss 0.5041699914638039\n",
            "2022-03-30 15:21:44.743716 Epoch 137, Training loss 0.5023256753526075\n",
            "2022-03-30 15:21:55.605758 Epoch 138, Training loss 0.49966210176420334\n",
            "2022-03-30 15:22:06.525076 Epoch 139, Training loss 0.5017446729799976\n",
            "2022-03-30 15:22:17.517424 Epoch 140, Training loss 0.5008128264447307\n",
            "2022-03-30 15:22:28.465611 Epoch 141, Training loss 0.49660163365132975\n",
            "2022-03-30 15:22:39.342308 Epoch 142, Training loss 0.49868984526151894\n",
            "2022-03-30 15:22:50.295691 Epoch 143, Training loss 0.49834863321326883\n",
            "2022-03-30 15:23:01.203659 Epoch 144, Training loss 0.4953119467438944\n",
            "2022-03-30 15:23:12.058858 Epoch 145, Training loss 0.4936095202708488\n",
            "2022-03-30 15:23:22.980109 Epoch 146, Training loss 0.4942217179385902\n",
            "2022-03-30 15:23:33.925319 Epoch 147, Training loss 0.490988834834922\n",
            "2022-03-30 15:23:44.911236 Epoch 148, Training loss 0.49263167482279147\n",
            "2022-03-30 15:23:55.951063 Epoch 149, Training loss 0.48943269576715387\n",
            "2022-03-30 15:24:06.940766 Epoch 150, Training loss 0.48841089930604487\n",
            "2022-03-30 15:24:17.792006 Epoch 151, Training loss 0.4868215649481625\n",
            "2022-03-30 15:24:28.667909 Epoch 152, Training loss 0.4849152695530516\n",
            "2022-03-30 15:24:39.637101 Epoch 153, Training loss 0.4835874158746141\n",
            "2022-03-30 15:24:50.911242 Epoch 154, Training loss 0.4850989770896904\n",
            "2022-03-30 15:25:01.932550 Epoch 155, Training loss 0.4829429281718286\n",
            "2022-03-30 15:25:12.984948 Epoch 156, Training loss 0.4800314438122008\n",
            "2022-03-30 15:25:23.954164 Epoch 157, Training loss 0.4792122710353273\n",
            "2022-03-30 15:25:34.841177 Epoch 158, Training loss 0.4832553399531433\n",
            "2022-03-30 15:25:45.633814 Epoch 159, Training loss 0.4793611015467083\n",
            "2022-03-30 15:25:56.534692 Epoch 160, Training loss 0.479492769178832\n",
            "2022-03-30 15:26:07.390589 Epoch 161, Training loss 0.4754373475318522\n",
            "2022-03-30 15:26:18.263341 Epoch 162, Training loss 0.47635048361080684\n",
            "2022-03-30 15:26:29.264799 Epoch 163, Training loss 0.47499521292002916\n",
            "2022-03-30 15:26:40.134748 Epoch 164, Training loss 0.47649312046025416\n",
            "2022-03-30 15:26:51.054065 Epoch 165, Training loss 0.473335251993383\n",
            "2022-03-30 15:27:01.991515 Epoch 166, Training loss 0.47051277068798497\n",
            "2022-03-30 15:27:12.896901 Epoch 167, Training loss 0.472720676885389\n",
            "2022-03-30 15:27:23.726778 Epoch 168, Training loss 0.4742358882942468\n",
            "2022-03-30 15:27:34.541976 Epoch 169, Training loss 0.4700844716233061\n",
            "2022-03-30 15:27:45.393247 Epoch 170, Training loss 0.469180548625529\n",
            "2022-03-30 15:27:56.222212 Epoch 171, Training loss 0.46942392997729504\n",
            "2022-03-30 15:28:07.217989 Epoch 172, Training loss 0.4681767937929734\n",
            "2022-03-30 15:28:18.006216 Epoch 173, Training loss 0.4677243512457289\n",
            "2022-03-30 15:28:28.775566 Epoch 174, Training loss 0.462929460170019\n",
            "2022-03-30 15:28:39.616483 Epoch 175, Training loss 0.4651122914479517\n",
            "2022-03-30 15:28:50.463743 Epoch 176, Training loss 0.4639074515618022\n",
            "2022-03-30 15:29:01.445217 Epoch 177, Training loss 0.4644551017057255\n",
            "2022-03-30 15:29:12.343246 Epoch 178, Training loss 0.4630651156920606\n",
            "2022-03-30 15:29:23.115292 Epoch 179, Training loss 0.46108863009211354\n",
            "2022-03-30 15:29:34.159277 Epoch 180, Training loss 0.46233543227700624\n",
            "2022-03-30 15:29:44.940636 Epoch 181, Training loss 0.4595619693322255\n",
            "2022-03-30 15:29:55.782196 Epoch 182, Training loss 0.4598546034234869\n",
            "2022-03-30 15:30:06.721788 Epoch 183, Training loss 0.4555978546743198\n",
            "2022-03-30 15:30:17.583071 Epoch 184, Training loss 0.45870780786666115\n",
            "2022-03-30 15:30:28.462701 Epoch 185, Training loss 0.45706891152255064\n",
            "2022-03-30 15:30:39.375236 Epoch 186, Training loss 0.4563245953577559\n",
            "2022-03-30 15:30:50.372181 Epoch 187, Training loss 0.454702249992534\n",
            "2022-03-30 15:31:01.263604 Epoch 188, Training loss 0.4536730021695652\n",
            "2022-03-30 15:31:12.100721 Epoch 189, Training loss 0.45591826470154323\n",
            "2022-03-30 15:31:22.856847 Epoch 190, Training loss 0.4538505960760824\n",
            "2022-03-30 15:31:33.694770 Epoch 191, Training loss 0.45361843545113684\n",
            "2022-03-30 15:31:44.543788 Epoch 192, Training loss 0.45266908437699616\n",
            "2022-03-30 15:31:55.394568 Epoch 193, Training loss 0.45076696729035026\n",
            "2022-03-30 15:32:06.303704 Epoch 194, Training loss 0.4514723455966891\n",
            "2022-03-30 15:32:17.112114 Epoch 195, Training loss 0.4503453919649734\n",
            "2022-03-30 15:32:27.901060 Epoch 196, Training loss 0.45027617300334183\n",
            "2022-03-30 15:32:39.215725 Epoch 197, Training loss 0.4485063199192057\n",
            "2022-03-30 15:32:50.112517 Epoch 198, Training loss 0.446231642209203\n",
            "2022-03-30 15:33:01.042326 Epoch 199, Training loss 0.44326288565574096\n",
            "2022-03-30 15:33:11.952508 Epoch 200, Training loss 0.44664115508270386\n",
            "2022-03-30 15:33:22.815525 Epoch 201, Training loss 0.4450011896469709\n",
            "2022-03-30 15:33:33.719720 Epoch 202, Training loss 0.44168465300594145\n",
            "2022-03-30 15:33:44.665666 Epoch 203, Training loss 0.44531438093813486\n",
            "2022-03-30 15:33:55.547195 Epoch 204, Training loss 0.44204171339188086\n",
            "2022-03-30 15:34:06.468299 Epoch 205, Training loss 0.43993044377821483\n",
            "2022-03-30 15:34:17.378279 Epoch 206, Training loss 0.4400289304497297\n",
            "2022-03-30 15:34:28.269509 Epoch 207, Training loss 0.4415342857313278\n",
            "2022-03-30 15:34:39.114389 Epoch 208, Training loss 0.44272521822272665\n",
            "2022-03-30 15:34:50.058850 Epoch 209, Training loss 0.4377758234281979\n",
            "2022-03-30 15:35:00.968931 Epoch 210, Training loss 0.4385080543225226\n",
            "2022-03-30 15:35:11.878450 Epoch 211, Training loss 0.435715993404236\n",
            "2022-03-30 15:35:22.917216 Epoch 212, Training loss 0.43749552391602864\n",
            "2022-03-30 15:35:33.734948 Epoch 213, Training loss 0.4401140428717484\n",
            "2022-03-30 15:35:44.571628 Epoch 214, Training loss 0.43655158270655386\n",
            "2022-03-30 15:35:55.497013 Epoch 215, Training loss 0.43470692468802336\n",
            "2022-03-30 15:36:06.512541 Epoch 216, Training loss 0.4384071064536529\n",
            "2022-03-30 15:36:17.485523 Epoch 217, Training loss 0.43443859448594513\n",
            "2022-03-30 15:36:28.216320 Epoch 218, Training loss 0.43522119581165825\n",
            "2022-03-30 15:36:39.164128 Epoch 219, Training loss 0.4345229767129549\n",
            "2022-03-30 15:36:50.414015 Epoch 220, Training loss 0.43299230302462494\n",
            "2022-03-30 15:37:01.430861 Epoch 221, Training loss 0.429966358691835\n",
            "2022-03-30 15:37:12.471100 Epoch 222, Training loss 0.428240743763459\n",
            "2022-03-30 15:37:23.482233 Epoch 223, Training loss 0.431328100240444\n",
            "2022-03-30 15:37:34.469260 Epoch 224, Training loss 0.42969809154339156\n",
            "2022-03-30 15:37:45.400909 Epoch 225, Training loss 0.429902494068036\n",
            "2022-03-30 15:37:56.182498 Epoch 226, Training loss 0.4306029723314068\n",
            "2022-03-30 15:38:07.372381 Epoch 227, Training loss 0.4292566171463798\n",
            "2022-03-30 15:38:18.399736 Epoch 228, Training loss 0.4276312422912444\n",
            "2022-03-30 15:38:29.388588 Epoch 229, Training loss 0.4281836811386411\n",
            "2022-03-30 15:38:40.277830 Epoch 230, Training loss 0.4250755681444312\n",
            "2022-03-30 15:38:51.325230 Epoch 231, Training loss 0.42968253901852366\n",
            "2022-03-30 15:39:02.207846 Epoch 232, Training loss 0.4248770464716665\n",
            "2022-03-30 15:39:13.016949 Epoch 233, Training loss 0.42591882714301427\n",
            "2022-03-30 15:39:23.939883 Epoch 234, Training loss 0.42469264233432463\n",
            "2022-03-30 15:39:34.965061 Epoch 235, Training loss 0.4244040156931371\n",
            "2022-03-30 15:39:45.916472 Epoch 236, Training loss 0.42542772408565294\n",
            "2022-03-30 15:39:56.804316 Epoch 237, Training loss 0.42694942799904156\n",
            "2022-03-30 15:40:07.805512 Epoch 238, Training loss 0.4228128826869723\n",
            "2022-03-30 15:40:18.844252 Epoch 239, Training loss 0.4204356085766307\n",
            "2022-03-30 15:40:29.671421 Epoch 240, Training loss 0.42360079330404093\n",
            "2022-03-30 15:40:40.570910 Epoch 241, Training loss 0.42052356386199935\n",
            "2022-03-30 15:40:51.544546 Epoch 242, Training loss 0.4198182348323905\n",
            "2022-03-30 15:41:02.574654 Epoch 243, Training loss 0.42124697444079173\n",
            "2022-03-30 15:41:13.489590 Epoch 244, Training loss 0.41878683929858\n",
            "2022-03-30 15:41:24.466514 Epoch 245, Training loss 0.41881117037952403\n",
            "2022-03-30 15:41:35.394609 Epoch 246, Training loss 0.4141507193903484\n",
            "2022-03-30 15:41:46.206397 Epoch 247, Training loss 0.4189677463506189\n",
            "2022-03-30 15:41:57.005977 Epoch 248, Training loss 0.4170384672672852\n",
            "2022-03-30 15:42:07.873509 Epoch 249, Training loss 0.415291260823112\n",
            "2022-03-30 15:42:18.943740 Epoch 250, Training loss 0.4155821524503286\n",
            "2022-03-30 15:42:29.983757 Epoch 251, Training loss 0.42038866052465973\n",
            "2022-03-30 15:42:41.108983 Epoch 252, Training loss 0.4148252708146639\n",
            "2022-03-30 15:42:52.047719 Epoch 253, Training loss 0.4168295511389937\n",
            "2022-03-30 15:43:03.099939 Epoch 254, Training loss 0.41319514599526325\n",
            "2022-03-30 15:43:13.811264 Epoch 255, Training loss 0.41553192465658995\n",
            "2022-03-30 15:43:24.823539 Epoch 256, Training loss 0.4124287036450013\n",
            "2022-03-30 15:43:35.757537 Epoch 257, Training loss 0.4129027687108425\n",
            "2022-03-30 15:43:46.711778 Epoch 258, Training loss 0.41555345045102526\n",
            "2022-03-30 15:43:57.620280 Epoch 259, Training loss 0.4125206110918004\n",
            "2022-03-30 15:44:08.543654 Epoch 260, Training loss 0.4085564224425789\n",
            "2022-03-30 15:44:19.432587 Epoch 261, Training loss 0.41192191101782155\n",
            "2022-03-30 15:44:30.316189 Epoch 262, Training loss 0.4089831098952257\n",
            "2022-03-30 15:44:40.892077 Epoch 263, Training loss 0.4069897540466255\n",
            "2022-03-30 15:44:51.686451 Epoch 264, Training loss 0.41011442221186656\n",
            "2022-03-30 15:45:02.821348 Epoch 265, Training loss 0.41107847499649236\n",
            "2022-03-30 15:45:13.623630 Epoch 266, Training loss 0.4115015898862153\n",
            "2022-03-30 15:45:24.354050 Epoch 267, Training loss 0.40735170210871247\n",
            "2022-03-30 15:45:35.232687 Epoch 268, Training loss 0.40914286143334627\n",
            "2022-03-30 15:45:46.138689 Epoch 269, Training loss 0.40963484870884426\n",
            "2022-03-30 15:45:56.943060 Epoch 270, Training loss 0.4077986040726647\n",
            "2022-03-30 15:46:08.088250 Epoch 271, Training loss 0.40786972386605297\n",
            "2022-03-30 15:46:18.913021 Epoch 272, Training loss 0.40815827848813724\n",
            "2022-03-30 15:46:30.057202 Epoch 273, Training loss 0.4044764209205232\n",
            "2022-03-30 15:46:41.003772 Epoch 274, Training loss 0.40348490348557375\n",
            "2022-03-30 15:46:51.945415 Epoch 275, Training loss 0.404296759887577\n",
            "2022-03-30 15:47:02.770629 Epoch 276, Training loss 0.4022756964539933\n",
            "2022-03-30 15:47:13.434221 Epoch 277, Training loss 0.4064341043610402\n",
            "2022-03-30 15:47:24.315070 Epoch 278, Training loss 0.40171761601172445\n",
            "2022-03-30 15:47:35.368345 Epoch 279, Training loss 0.4054197574896581\n",
            "2022-03-30 15:47:46.299979 Epoch 280, Training loss 0.40123326739158166\n",
            "2022-03-30 15:47:57.101316 Epoch 281, Training loss 0.40124095938242305\n",
            "2022-03-30 15:48:08.008192 Epoch 282, Training loss 0.4057898214825279\n",
            "2022-03-30 15:48:18.967009 Epoch 283, Training loss 0.4028562074884429\n",
            "2022-03-30 15:48:29.792898 Epoch 284, Training loss 0.4024388769741558\n",
            "2022-03-30 15:48:40.600907 Epoch 285, Training loss 0.4011030084527362\n",
            "2022-03-30 15:48:51.458719 Epoch 286, Training loss 0.39901595654161387\n",
            "2022-03-30 15:49:02.643182 Epoch 287, Training loss 0.4002588281164999\n",
            "2022-03-30 15:49:13.700757 Epoch 288, Training loss 0.40131128072509986\n",
            "2022-03-30 15:49:24.845284 Epoch 289, Training loss 0.4037212198957458\n",
            "2022-03-30 15:49:35.804115 Epoch 290, Training loss 0.3981615993029931\n",
            "2022-03-30 15:49:46.824682 Epoch 291, Training loss 0.39880313395577316\n",
            "2022-03-30 15:49:57.562957 Epoch 292, Training loss 0.3959845649388135\n",
            "2022-03-30 15:50:08.510906 Epoch 293, Training loss 0.39768407934004696\n",
            "2022-03-30 15:50:19.507076 Epoch 294, Training loss 0.39597193502327976\n",
            "2022-03-30 15:50:30.597775 Epoch 295, Training loss 0.3961890802035094\n",
            "2022-03-30 15:50:41.555955 Epoch 296, Training loss 0.3948050790735523\n",
            "2022-03-30 15:50:52.509779 Epoch 297, Training loss 0.39773797836450053\n",
            "2022-03-30 15:51:03.417405 Epoch 298, Training loss 0.3985724752135289\n",
            "2022-03-30 15:51:14.467871 Epoch 299, Training loss 0.39597976272520813\n",
            "2022-03-30 15:51:25.786951 Epoch 300, Training loss 0.3975446016511039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRpeE77JxsJj",
        "outputId": "aa6208dc-7f2a-4aac-b513-7a037dc5c1a9"
      },
      "id": "nRpeE77JxsJj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.76\n",
            "Accuracy val: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.b. Extend your CNN by adding one more additional convolution layer followed by an activation function and pooling function. You also need to adjust your fully connected layer properly with respect to intermediate feature dimensions. Train your network for 300 epochs. Report your training time, loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare your model size and accuracy over the baseline implementation in Problem2.a. Do you see any over-fitting? Make sure to submit your code by providing the GitHub URL of your course repository for this course (30pt)**"
      ],
      "metadata": {
        "id": "QzhX4wuBiafd"
      },
      "id": "QzhX4wuBiafd"
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "          super(NetRes, self).__init__()\n",
        "          self.conv = nn.Conv2d(n_chans1, n_chans1, \n",
        "            kernel_size=3, padding=1, bias=False)\n",
        "          self.batch_norm = nn.BatchNorm2d(num_features=\n",
        "                                     n_chans1)\n",
        "          torch.nn.init.kaiming_normal_(self.conv.weight, \n",
        "                            nonlinearity='relu')\n",
        "          torch.nn.init.constant_(self.batch_norm.weight, \n",
        "                            0.5)\n",
        "          torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "        \n",
        "    def forward(self, x):\n",
        "     out = self.conv(x)\n",
        "     out = torch.relu(out)\n",
        "     return out + x"
      ],
      "metadata": {
        "id": "cqVvRy67_Hy1"
      },
      "id": "cqVvRy67_Hy1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, \n",
        "                        batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs =300,\n",
        "    optimizer= optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcCrjQoG_5MJ",
        "outputId": "11a4adc7-6cf8-4f56-dfe3-6568cf850e3a"
      },
      "id": "zcCrjQoG_5MJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-30 22:43:00.080416 Epoch 1, Training loss 2.0433139910783304\n",
            "2022-03-30 22:43:09.056253 Epoch 2, Training loss 1.7821431681323234\n",
            "2022-03-30 22:43:18.483246 Epoch 3, Training loss 1.6077832204606526\n",
            "2022-03-30 22:43:27.734464 Epoch 4, Training loss 1.5257196385232383\n",
            "2022-03-30 22:43:37.226082 Epoch 5, Training loss 1.4694439521073686\n",
            "2022-03-30 22:43:46.696314 Epoch 6, Training loss 1.4247727144099867\n",
            "2022-03-30 22:43:56.168772 Epoch 7, Training loss 1.385382987196793\n",
            "2022-03-30 22:44:05.417752 Epoch 8, Training loss 1.345282492735197\n",
            "2022-03-30 22:44:14.700885 Epoch 9, Training loss 1.3018881581780855\n",
            "2022-03-30 22:44:23.814807 Epoch 10, Training loss 1.2568906674452145\n",
            "2022-03-30 22:44:33.032870 Epoch 11, Training loss 1.2209365199441495\n",
            "2022-03-30 22:44:42.461687 Epoch 12, Training loss 1.1908568870990783\n",
            "2022-03-30 22:44:51.618654 Epoch 13, Training loss 1.1631897738979906\n",
            "2022-03-30 22:45:00.702190 Epoch 14, Training loss 1.1378452889907085\n",
            "2022-03-30 22:45:10.113566 Epoch 15, Training loss 1.1176696303098097\n",
            "2022-03-30 22:45:19.350377 Epoch 16, Training loss 1.0978669314585683\n",
            "2022-03-30 22:45:28.530513 Epoch 17, Training loss 1.0796291674951763\n",
            "2022-03-30 22:45:37.844822 Epoch 18, Training loss 1.0625739611323228\n",
            "2022-03-30 22:45:46.943128 Epoch 19, Training loss 1.0493576762926242\n",
            "2022-03-30 22:45:56.133728 Epoch 20, Training loss 1.0355397427021085\n",
            "2022-03-30 22:46:05.113787 Epoch 21, Training loss 1.0240520783854872\n",
            "2022-03-30 22:46:14.596762 Epoch 22, Training loss 1.0116074271214284\n",
            "2022-03-30 22:46:23.654390 Epoch 23, Training loss 1.0023854921388504\n",
            "2022-03-30 22:46:32.900251 Epoch 24, Training loss 0.9945218564604249\n",
            "2022-03-30 22:46:42.210421 Epoch 25, Training loss 0.9858207536475433\n",
            "2022-03-30 22:46:51.324182 Epoch 26, Training loss 0.9765059474636527\n",
            "2022-03-30 22:47:00.604026 Epoch 27, Training loss 0.9687950844350068\n",
            "2022-03-30 22:47:09.578107 Epoch 28, Training loss 0.9626021958373087\n",
            "2022-03-30 22:47:18.623721 Epoch 29, Training loss 0.9549488700411813\n",
            "2022-03-30 22:47:27.815689 Epoch 30, Training loss 0.9483764814141461\n",
            "2022-03-30 22:47:37.001699 Epoch 31, Training loss 0.9409043338445141\n",
            "2022-03-30 22:47:46.146490 Epoch 32, Training loss 0.9335385696662356\n",
            "2022-03-30 22:47:55.261943 Epoch 33, Training loss 0.9281366084847609\n",
            "2022-03-30 22:48:04.506132 Epoch 34, Training loss 0.9213916096845856\n",
            "2022-03-30 22:48:13.778300 Epoch 35, Training loss 0.9161236245004113\n",
            "2022-03-30 22:48:22.846052 Epoch 36, Training loss 0.9103046411748432\n",
            "2022-03-30 22:48:31.769517 Epoch 37, Training loss 0.9036654845985306\n",
            "2022-03-30 22:48:41.018708 Epoch 38, Training loss 0.8990452277767079\n",
            "2022-03-30 22:48:50.306903 Epoch 39, Training loss 0.8942073728422375\n",
            "2022-03-30 22:48:59.597068 Epoch 40, Training loss 0.8886247247533725\n",
            "2022-03-30 22:49:08.720037 Epoch 41, Training loss 0.8837815003321908\n",
            "2022-03-30 22:49:18.083312 Epoch 42, Training loss 0.8786501556711124\n",
            "2022-03-30 22:49:27.208097 Epoch 43, Training loss 0.8728969801798501\n",
            "2022-03-30 22:49:36.590625 Epoch 44, Training loss 0.8692923372663806\n",
            "2022-03-30 22:49:45.943763 Epoch 45, Training loss 0.8651089654173083\n",
            "2022-03-30 22:49:55.092791 Epoch 46, Training loss 0.860038573937038\n",
            "2022-03-30 22:50:04.700714 Epoch 47, Training loss 0.8547497316242179\n",
            "2022-03-30 22:50:13.899370 Epoch 48, Training loss 0.8512534232395689\n",
            "2022-03-30 22:50:23.237794 Epoch 49, Training loss 0.847132809745991\n",
            "2022-03-30 22:50:32.451571 Epoch 50, Training loss 0.8436875078455567\n",
            "2022-03-30 22:50:41.611113 Epoch 51, Training loss 0.8378376843755507\n",
            "2022-03-30 22:50:50.881679 Epoch 52, Training loss 0.8340969494236704\n",
            "2022-03-30 22:51:00.215010 Epoch 53, Training loss 0.8309336472350313\n",
            "2022-03-30 22:51:09.145886 Epoch 54, Training loss 0.8274258290943892\n",
            "2022-03-30 22:51:18.366420 Epoch 55, Training loss 0.8236568352145612\n",
            "2022-03-30 22:51:27.719700 Epoch 56, Training loss 0.8204166014176195\n",
            "2022-03-30 22:51:36.919833 Epoch 57, Training loss 0.8161967308320048\n",
            "2022-03-30 22:51:46.128193 Epoch 58, Training loss 0.8126945015033493\n",
            "2022-03-30 22:51:55.479887 Epoch 59, Training loss 0.8101605705135618\n",
            "2022-03-30 22:52:04.755038 Epoch 60, Training loss 0.8062197367858399\n",
            "2022-03-30 22:52:14.135680 Epoch 61, Training loss 0.8029446828243373\n",
            "2022-03-30 22:52:23.435584 Epoch 62, Training loss 0.7986018473992262\n",
            "2022-03-30 22:52:32.427472 Epoch 63, Training loss 0.7963255248639894\n",
            "2022-03-30 22:52:41.524592 Epoch 64, Training loss 0.7947031071652537\n",
            "2022-03-30 22:52:51.223371 Epoch 65, Training loss 0.7908895333938282\n",
            "2022-03-30 22:53:00.330837 Epoch 66, Training loss 0.7875865857162134\n",
            "2022-03-30 22:53:09.640786 Epoch 67, Training loss 0.7848909802144141\n",
            "2022-03-30 22:53:18.766025 Epoch 68, Training loss 0.7809438404753385\n",
            "2022-03-30 22:53:27.915698 Epoch 69, Training loss 0.7788766341288681\n",
            "2022-03-30 22:53:37.358969 Epoch 70, Training loss 0.7750676541072329\n",
            "2022-03-30 22:53:46.493613 Epoch 71, Training loss 0.7752228687181497\n",
            "2022-03-30 22:53:55.475758 Epoch 72, Training loss 0.7703852459140446\n",
            "2022-03-30 22:54:04.679953 Epoch 73, Training loss 0.7691448444448163\n",
            "2022-03-30 22:54:13.709731 Epoch 74, Training loss 0.7639342988543498\n",
            "2022-03-30 22:54:22.916699 Epoch 75, Training loss 0.7631786786534293\n",
            "2022-03-30 22:54:32.094254 Epoch 76, Training loss 0.7601490673963981\n",
            "2022-03-30 22:54:41.457917 Epoch 77, Training loss 0.7574696513393041\n",
            "2022-03-30 22:54:50.732058 Epoch 78, Training loss 0.7557284774454048\n",
            "2022-03-30 22:54:59.988893 Epoch 79, Training loss 0.7532423489996235\n",
            "2022-03-30 22:55:09.204720 Epoch 80, Training loss 0.7510043770989494\n",
            "2022-03-30 22:55:18.282957 Epoch 81, Training loss 0.7471775234584004\n",
            "2022-03-30 22:55:27.654391 Epoch 82, Training loss 0.7452909616786806\n",
            "2022-03-30 22:55:37.115008 Epoch 83, Training loss 0.7441483934593323\n",
            "2022-03-30 22:55:46.123200 Epoch 84, Training loss 0.7424836707923114\n",
            "2022-03-30 22:55:55.312747 Epoch 85, Training loss 0.7400352948385737\n",
            "2022-03-30 22:56:04.737999 Epoch 86, Training loss 0.7361532463442029\n",
            "2022-03-30 22:56:14.042589 Epoch 87, Training loss 0.7352834117915624\n",
            "2022-03-30 22:56:23.478480 Epoch 88, Training loss 0.7325031606818709\n",
            "2022-03-30 22:56:32.577496 Epoch 89, Training loss 0.731102618072039\n",
            "2022-03-30 22:56:41.606502 Epoch 90, Training loss 0.7280676654156517\n",
            "2022-03-30 22:56:51.017957 Epoch 91, Training loss 0.7279561369696541\n",
            "2022-03-30 22:57:00.319020 Epoch 92, Training loss 0.7252229848100097\n",
            "2022-03-30 22:57:09.715076 Epoch 93, Training loss 0.7242615827742744\n",
            "2022-03-30 22:57:19.020494 Epoch 94, Training loss 0.7217815388041688\n",
            "2022-03-30 22:57:28.163505 Epoch 95, Training loss 0.7174776816916892\n",
            "2022-03-30 22:57:37.491222 Epoch 96, Training loss 0.7171816953536495\n",
            "2022-03-30 22:57:46.865073 Epoch 97, Training loss 0.7142757502815608\n",
            "2022-03-30 22:57:55.789020 Epoch 98, Training loss 0.7137278524201239\n",
            "2022-03-30 22:58:05.052771 Epoch 99, Training loss 0.7108313805230743\n",
            "2022-03-30 22:58:14.290933 Epoch 100, Training loss 0.7094357596791309\n",
            "2022-03-30 22:58:23.739718 Epoch 101, Training loss 0.7073342303180938\n",
            "2022-03-30 22:58:33.269925 Epoch 102, Training loss 0.7064153584830292\n",
            "2022-03-30 22:58:42.521796 Epoch 103, Training loss 0.7040979778751388\n",
            "2022-03-30 22:58:51.809997 Epoch 104, Training loss 0.701848077880757\n",
            "2022-03-30 22:59:01.177838 Epoch 105, Training loss 0.6985227936292853\n",
            "2022-03-30 22:59:10.297083 Epoch 106, Training loss 0.6997520603868358\n",
            "2022-03-30 22:59:19.280521 Epoch 107, Training loss 0.6987103859863013\n",
            "2022-03-30 22:59:28.453067 Epoch 108, Training loss 0.6948021045502495\n",
            "2022-03-30 22:59:37.760202 Epoch 109, Training loss 0.693694171393314\n",
            "2022-03-30 22:59:47.297715 Epoch 110, Training loss 0.6909396781793335\n",
            "2022-03-30 22:59:56.808172 Epoch 111, Training loss 0.6903244415893579\n",
            "2022-03-30 23:00:06.311354 Epoch 112, Training loss 0.6884206080680613\n",
            "2022-03-30 23:00:15.630185 Epoch 113, Training loss 0.6864704547635735\n",
            "2022-03-30 23:00:25.217912 Epoch 114, Training loss 0.6855922324197067\n",
            "2022-03-30 23:00:34.369699 Epoch 115, Training loss 0.6846609506994257\n",
            "2022-03-30 23:00:43.725171 Epoch 116, Training loss 0.6837098292072715\n",
            "2022-03-30 23:00:53.261332 Epoch 117, Training loss 0.6818309549404227\n",
            "2022-03-30 23:01:02.822727 Epoch 118, Training loss 0.679618110780216\n",
            "2022-03-30 23:01:12.286369 Epoch 119, Training loss 0.6778591521027143\n",
            "2022-03-30 23:01:21.761415 Epoch 120, Training loss 0.6757374839938205\n",
            "2022-03-30 23:01:31.489234 Epoch 121, Training loss 0.6746902779087691\n",
            "2022-03-30 23:01:40.903354 Epoch 122, Training loss 0.6734041365439934\n",
            "2022-03-30 23:01:50.281539 Epoch 123, Training loss 0.6708694379729079\n",
            "2022-03-30 23:01:59.289142 Epoch 124, Training loss 0.6691963270954464\n",
            "2022-03-30 23:02:08.843166 Epoch 125, Training loss 0.667723171927435\n",
            "2022-03-30 23:02:18.120944 Epoch 126, Training loss 0.6666456272306345\n",
            "2022-03-30 23:02:27.333792 Epoch 127, Training loss 0.6657290686960415\n",
            "2022-03-30 23:02:36.655677 Epoch 128, Training loss 0.665571184414427\n",
            "2022-03-30 23:02:45.780466 Epoch 129, Training loss 0.6643440270286691\n",
            "2022-03-30 23:02:54.795083 Epoch 130, Training loss 0.6619411431005239\n",
            "2022-03-30 23:03:04.236758 Epoch 131, Training loss 0.6613091792901764\n",
            "2022-03-30 23:03:13.431529 Epoch 132, Training loss 0.6605622855293781\n",
            "2022-03-30 23:03:22.491390 Epoch 133, Training loss 0.657961355946253\n",
            "2022-03-30 23:03:31.900766 Epoch 134, Training loss 0.658692389650418\n",
            "2022-03-30 23:03:41.221391 Epoch 135, Training loss 0.6539594350797137\n",
            "2022-03-30 23:03:50.281007 Epoch 136, Training loss 0.6557859211702786\n",
            "2022-03-30 23:03:59.526884 Epoch 137, Training loss 0.6521818451869213\n",
            "2022-03-30 23:04:08.876017 Epoch 138, Training loss 0.6518422460083462\n",
            "2022-03-30 23:04:18.414210 Epoch 139, Training loss 0.6495376244149245\n",
            "2022-03-30 23:04:27.649114 Epoch 140, Training loss 0.6485660897039086\n",
            "2022-03-30 23:04:36.634161 Epoch 141, Training loss 0.6478255587389402\n",
            "2022-03-30 23:04:45.891578 Epoch 142, Training loss 0.6458351155528632\n",
            "2022-03-30 23:04:55.315617 Epoch 143, Training loss 0.6470191897181294\n",
            "2022-03-30 23:05:04.801143 Epoch 144, Training loss 0.6436109455954998\n",
            "2022-03-30 23:05:14.110273 Epoch 145, Training loss 0.6419577825328578\n",
            "2022-03-30 23:05:23.296836 Epoch 146, Training loss 0.6402618114448264\n",
            "2022-03-30 23:05:32.539559 Epoch 147, Training loss 0.6382840116081945\n",
            "2022-03-30 23:05:41.697909 Epoch 148, Training loss 0.6401568356987155\n",
            "2022-03-30 23:05:50.844915 Epoch 149, Training loss 0.6383799130235182\n",
            "2022-03-30 23:05:59.751625 Epoch 150, Training loss 0.6371194199299264\n",
            "2022-03-30 23:06:09.074615 Epoch 151, Training loss 0.6345545250131651\n",
            "2022-03-30 23:06:18.165725 Epoch 152, Training loss 0.6350425514952301\n",
            "2022-03-30 23:06:27.625637 Epoch 153, Training loss 0.6343670748460019\n",
            "2022-03-30 23:06:36.913747 Epoch 154, Training loss 0.631985672370857\n",
            "2022-03-30 23:06:46.155073 Epoch 155, Training loss 0.6322033633585171\n",
            "2022-03-30 23:06:55.429092 Epoch 156, Training loss 0.6320968296216882\n",
            "2022-03-30 23:07:04.441035 Epoch 157, Training loss 0.6315436700116033\n",
            "2022-03-30 23:07:13.442192 Epoch 158, Training loss 0.6279991752351336\n",
            "2022-03-30 23:07:22.390538 Epoch 159, Training loss 0.6288218169718447\n",
            "2022-03-30 23:07:31.817397 Epoch 160, Training loss 0.6263497845291177\n",
            "2022-03-30 23:07:41.095944 Epoch 161, Training loss 0.6247238881905061\n",
            "2022-03-30 23:07:50.376676 Epoch 162, Training loss 0.6240509394032266\n",
            "2022-03-30 23:07:59.299367 Epoch 163, Training loss 0.6236460294641192\n",
            "2022-03-30 23:08:08.543029 Epoch 164, Training loss 0.6214037167904017\n",
            "2022-03-30 23:08:17.784297 Epoch 165, Training loss 0.6219677575637618\n",
            "2022-03-30 23:08:27.058335 Epoch 166, Training loss 0.6225644576808681\n",
            "2022-03-30 23:08:36.054779 Epoch 167, Training loss 0.6206196258058938\n",
            "2022-03-30 23:08:44.976305 Epoch 168, Training loss 0.618597805157037\n",
            "2022-03-30 23:08:54.291795 Epoch 169, Training loss 0.6185371919589884\n",
            "2022-03-30 23:09:03.769563 Epoch 170, Training loss 0.6163179713594334\n",
            "2022-03-30 23:09:12.804395 Epoch 171, Training loss 0.6162333645860253\n",
            "2022-03-30 23:09:22.204828 Epoch 172, Training loss 0.6158811532323013\n",
            "2022-03-30 23:09:31.663557 Epoch 173, Training loss 0.6141559471330984\n",
            "2022-03-30 23:09:40.905576 Epoch 174, Training loss 0.6132172211204343\n",
            "2022-03-30 23:09:50.351788 Epoch 175, Training loss 0.6116385172928691\n",
            "2022-03-30 23:09:59.392315 Epoch 176, Training loss 0.612828500892805\n",
            "2022-03-30 23:10:08.618337 Epoch 177, Training loss 0.609975494120432\n",
            "2022-03-30 23:10:17.941026 Epoch 178, Training loss 0.6073844135569795\n",
            "2022-03-30 23:10:27.225170 Epoch 179, Training loss 0.6080852230948866\n",
            "2022-03-30 23:10:36.228226 Epoch 180, Training loss 0.6071884468235933\n",
            "2022-03-30 23:10:45.579535 Epoch 181, Training loss 0.6079349441220389\n",
            "2022-03-30 23:10:55.033209 Epoch 182, Training loss 0.6067444875340937\n",
            "2022-03-30 23:11:04.447010 Epoch 183, Training loss 0.6055729811834862\n",
            "2022-03-30 23:11:13.805059 Epoch 184, Training loss 0.6048378331200851\n",
            "2022-03-30 23:11:22.738540 Epoch 185, Training loss 0.6042945006162005\n",
            "2022-03-30 23:11:31.943602 Epoch 186, Training loss 0.602525477061796\n",
            "2022-03-30 23:11:41.098096 Epoch 187, Training loss 0.6012481242951835\n",
            "2022-03-30 23:11:50.329885 Epoch 188, Training loss 0.6002007846332267\n",
            "2022-03-30 23:11:59.537652 Epoch 189, Training loss 0.6006655039461067\n",
            "2022-03-30 23:12:08.885932 Epoch 190, Training loss 0.5979652619346634\n",
            "2022-03-30 23:12:18.450803 Epoch 191, Training loss 0.5979774712449144\n",
            "2022-03-30 23:12:27.768493 Epoch 192, Training loss 0.5967068971346712\n",
            "2022-03-30 23:12:36.974221 Epoch 193, Training loss 0.5969646829930718\n",
            "2022-03-30 23:12:45.873749 Epoch 194, Training loss 0.5963860669785448\n",
            "2022-03-30 23:12:55.189119 Epoch 195, Training loss 0.5951364158516954\n",
            "2022-03-30 23:13:04.403230 Epoch 196, Training loss 0.5956324474204837\n",
            "2022-03-30 23:13:13.492355 Epoch 197, Training loss 0.5929741596855471\n",
            "2022-03-30 23:13:22.756330 Epoch 198, Training loss 0.5931024651621919\n",
            "2022-03-30 23:13:31.920815 Epoch 199, Training loss 0.5912405831353439\n",
            "2022-03-30 23:13:41.549971 Epoch 200, Training loss 0.5919203138564859\n",
            "2022-03-30 23:13:51.208087 Epoch 201, Training loss 0.5897567983326095\n",
            "2022-03-30 23:14:00.371740 Epoch 202, Training loss 0.5904217650518393\n",
            "2022-03-30 23:14:09.550716 Epoch 203, Training loss 0.5891862098137131\n",
            "2022-03-30 23:14:19.273338 Epoch 204, Training loss 0.5880626876031041\n",
            "2022-03-30 23:14:28.779333 Epoch 205, Training loss 0.5865559957140242\n",
            "2022-03-30 23:14:38.232608 Epoch 206, Training loss 0.5874489337739432\n",
            "2022-03-30 23:14:47.440619 Epoch 207, Training loss 0.585760813921004\n",
            "2022-03-30 23:14:56.761110 Epoch 208, Training loss 0.5857396920776123\n",
            "2022-03-30 23:15:06.299607 Epoch 209, Training loss 0.583949885690761\n",
            "2022-03-30 23:15:15.763767 Epoch 210, Training loss 0.5818769527060906\n",
            "2022-03-30 23:15:24.846812 Epoch 211, Training loss 0.5828627019434633\n",
            "2022-03-30 23:15:34.272614 Epoch 212, Training loss 0.5818449293865877\n",
            "2022-03-30 23:15:43.512112 Epoch 213, Training loss 0.5818081763775452\n",
            "2022-03-30 23:15:53.102205 Epoch 214, Training loss 0.5800911734628555\n",
            "2022-03-30 23:16:02.383313 Epoch 215, Training loss 0.5793580979566135\n",
            "2022-03-30 23:16:11.665722 Epoch 216, Training loss 0.5804130641167121\n",
            "2022-03-30 23:16:20.937588 Epoch 217, Training loss 0.5778535846096781\n",
            "2022-03-30 23:16:30.241200 Epoch 218, Training loss 0.5770543223756659\n",
            "2022-03-30 23:16:39.550693 Epoch 219, Training loss 0.5796453917727751\n",
            "2022-03-30 23:16:48.769069 Epoch 220, Training loss 0.5758118692147153\n",
            "2022-03-30 23:16:58.002778 Epoch 221, Training loss 0.5762917562518888\n",
            "2022-03-30 23:17:07.557231 Epoch 222, Training loss 0.5731984387959361\n",
            "2022-03-30 23:17:16.681579 Epoch 223, Training loss 0.5734146421827624\n",
            "2022-03-30 23:17:26.093916 Epoch 224, Training loss 0.5730488616639696\n",
            "2022-03-30 23:17:35.538282 Epoch 225, Training loss 0.5737648037693385\n",
            "2022-03-30 23:17:44.754351 Epoch 226, Training loss 0.5725325165540361\n",
            "2022-03-30 23:17:54.217899 Epoch 227, Training loss 0.5727173244709249\n",
            "2022-03-30 23:18:03.728632 Epoch 228, Training loss 0.5715735106898086\n",
            "2022-03-30 23:18:12.856531 Epoch 229, Training loss 0.571348921569717\n",
            "2022-03-30 23:18:22.276724 Epoch 230, Training loss 0.5693457077455033\n",
            "2022-03-30 23:18:31.792916 Epoch 231, Training loss 0.571553928010604\n",
            "2022-03-30 23:18:41.323629 Epoch 232, Training loss 0.5683840891284406\n",
            "2022-03-30 23:18:50.525160 Epoch 233, Training loss 0.5680722864082707\n",
            "2022-03-30 23:18:59.859738 Epoch 234, Training loss 0.567982126894357\n",
            "2022-03-30 23:19:09.540638 Epoch 235, Training loss 0.5676830548916936\n",
            "2022-03-30 23:19:18.965039 Epoch 236, Training loss 0.5664710194787101\n",
            "2022-03-30 23:19:27.978481 Epoch 237, Training loss 0.5672021866835597\n",
            "2022-03-30 23:19:37.117588 Epoch 238, Training loss 0.5652240045997493\n",
            "2022-03-30 23:19:46.455730 Epoch 239, Training loss 0.5651613459029161\n",
            "2022-03-30 23:19:55.638655 Epoch 240, Training loss 0.5656452377891297\n",
            "2022-03-30 23:20:05.461882 Epoch 241, Training loss 0.5619634669607557\n",
            "2022-03-30 23:20:14.910464 Epoch 242, Training loss 0.56164596807164\n",
            "2022-03-30 23:20:24.304486 Epoch 243, Training loss 0.5628333381755882\n",
            "2022-03-30 23:20:33.826646 Epoch 244, Training loss 0.5607728713461201\n",
            "2022-03-30 23:20:43.004579 Epoch 245, Training loss 0.5598784361577704\n",
            "2022-03-30 23:20:52.343190 Epoch 246, Training loss 0.5590540629137507\n",
            "2022-03-30 23:21:01.660615 Epoch 247, Training loss 0.5591104510800003\n",
            "2022-03-30 23:21:10.987873 Epoch 248, Training loss 0.5580162590421984\n",
            "2022-03-30 23:21:20.377136 Epoch 249, Training loss 0.5586595401891967\n",
            "2022-03-30 23:21:29.620882 Epoch 250, Training loss 0.5583848787466889\n",
            "2022-03-30 23:21:39.031957 Epoch 251, Training loss 0.5564345938089253\n",
            "2022-03-30 23:21:48.601535 Epoch 252, Training loss 0.5554804444465491\n",
            "2022-03-30 23:21:58.088458 Epoch 253, Training loss 0.5566776196288941\n",
            "2022-03-30 23:22:07.111549 Epoch 254, Training loss 0.556271286762279\n",
            "2022-03-30 23:22:16.469655 Epoch 255, Training loss 0.5552512849383342\n",
            "2022-03-30 23:22:25.813677 Epoch 256, Training loss 0.5535686547722658\n",
            "2022-03-30 23:22:35.185874 Epoch 257, Training loss 0.5554126422194874\n",
            "2022-03-30 23:22:44.626510 Epoch 258, Training loss 0.5530918349542886\n",
            "2022-03-30 23:22:53.993950 Epoch 259, Training loss 0.5517938669647098\n",
            "2022-03-30 23:23:03.385109 Epoch 260, Training loss 0.5524163420890909\n",
            "2022-03-30 23:23:12.779304 Epoch 261, Training loss 0.5521543891457341\n",
            "2022-03-30 23:23:21.903780 Epoch 262, Training loss 0.5487461045117634\n",
            "2022-03-30 23:23:30.979565 Epoch 263, Training loss 0.5511978336078737\n",
            "2022-03-30 23:23:40.446069 Epoch 264, Training loss 0.5504791845217385\n",
            "2022-03-30 23:23:49.612146 Epoch 265, Training loss 0.5503946815610237\n",
            "2022-03-30 23:23:58.937957 Epoch 266, Training loss 0.5494481503125042\n",
            "2022-03-30 23:24:08.200004 Epoch 267, Training loss 0.5497199339634927\n",
            "2022-03-30 23:24:17.530414 Epoch 268, Training loss 0.5464772621879492\n",
            "2022-03-30 23:24:26.915202 Epoch 269, Training loss 0.5471343129987607\n",
            "2022-03-30 23:24:36.179795 Epoch 270, Training loss 0.5473961013624126\n",
            "2022-03-30 23:24:45.334759 Epoch 271, Training loss 0.5459113548631254\n",
            "2022-03-30 23:24:54.436497 Epoch 272, Training loss 0.5471358493237239\n",
            "2022-03-30 23:25:03.710822 Epoch 273, Training loss 0.5444522787771566\n",
            "2022-03-30 23:25:13.340132 Epoch 274, Training loss 0.5463207022613271\n",
            "2022-03-30 23:25:22.799120 Epoch 275, Training loss 0.5437561671447266\n",
            "2022-03-30 23:25:32.054039 Epoch 276, Training loss 0.5443910781074973\n",
            "2022-03-30 23:25:41.423917 Epoch 277, Training loss 0.5410070780216886\n",
            "2022-03-30 23:25:50.783129 Epoch 278, Training loss 0.5434590305971063\n",
            "2022-03-30 23:26:00.213354 Epoch 279, Training loss 0.5403318071304379\n",
            "2022-03-30 23:26:09.283702 Epoch 280, Training loss 0.5439352125234311\n",
            "2022-03-30 23:26:18.886573 Epoch 281, Training loss 0.542843020080453\n",
            "2022-03-30 23:26:28.413008 Epoch 282, Training loss 0.5427252830523054\n",
            "2022-03-30 23:26:37.775325 Epoch 283, Training loss 0.5408545179325907\n",
            "2022-03-30 23:26:47.108169 Epoch 284, Training loss 0.5403940593990524\n",
            "2022-03-30 23:26:56.492326 Epoch 285, Training loss 0.5391279345430682\n",
            "2022-03-30 23:27:06.082627 Epoch 286, Training loss 0.5390649620834214\n",
            "2022-03-30 23:27:15.380539 Epoch 287, Training loss 0.537678191919461\n",
            "2022-03-30 23:27:24.560825 Epoch 288, Training loss 0.5390832826609502\n",
            "2022-03-30 23:27:33.646818 Epoch 289, Training loss 0.5377432840407047\n",
            "2022-03-30 23:27:42.867400 Epoch 290, Training loss 0.538158214915439\n",
            "2022-03-30 23:27:52.725952 Epoch 291, Training loss 0.5382662101855973\n",
            "2022-03-30 23:28:02.412506 Epoch 292, Training loss 0.535513639716846\n",
            "2022-03-30 23:28:11.627593 Epoch 293, Training loss 0.5368290699046591\n",
            "2022-03-30 23:28:21.195719 Epoch 294, Training loss 0.5356248577156335\n",
            "2022-03-30 23:28:30.358643 Epoch 295, Training loss 0.534610606596598\n",
            "2022-03-30 23:28:39.722728 Epoch 296, Training loss 0.5353069868691437\n",
            "2022-03-30 23:28:48.970181 Epoch 297, Training loss 0.5348970570680126\n",
            "2022-03-30 23:28:58.191555 Epoch 298, Training loss 0.5359941850537839\n",
            "2022-03-30 23:29:07.573335 Epoch 299, Training loss 0.5317887856489252\n",
            "2022-03-30 23:29:16.995635 Epoch 300, Training loss 0.5321382847817048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EixdwMve-07g",
        "outputId": "df156a94-764e-4609-b0d9-14bc2ec2a5e4"
      },
      "id": "EixdwMve-07g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://github.com/Cole-Fredrick/REAL_TIME_ML/blob/main/Homework_3.ipynb)"
      ],
      "metadata": {
        "id": "AvcUWbolmcWl"
      },
      "id": "AvcUWbolmcWl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "55fda10dd22042d08466bb4ee0e516b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46ef21000ec94c45856f39dceb4b8aef",
              "IPY_MODEL_1b2c4106dd88477cbbd39ac3654de724",
              "IPY_MODEL_b33fe9614f534ef28c002a2b09ebbb78"
            ],
            "layout": "IPY_MODEL_c524a45ee05846eb9018ae78add1b0cf"
          }
        },
        "46ef21000ec94c45856f39dceb4b8aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6544e89cbd444bbb843d26ca18a045",
            "placeholder": "​",
            "style": "IPY_MODEL_c3bca4c09b6e4ed0a2d519bea7550e42",
            "value": ""
          }
        },
        "1b2c4106dd88477cbbd39ac3654de724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372c6a058c3a484c9af8acbbe5738862",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ce5a74ea39d4c469f54d0bfb8746023",
            "value": 170498071
          }
        },
        "b33fe9614f534ef28c002a2b09ebbb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c18b4b364924a5d913a19afa3f54413",
            "placeholder": "​",
            "style": "IPY_MODEL_e88e584e07af4187a81e1c4973d99b79",
            "value": " 170499072/? [00:03&lt;00:00, 53393772.71it/s]"
          }
        },
        "c524a45ee05846eb9018ae78add1b0cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f6544e89cbd444bbb843d26ca18a045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3bca4c09b6e4ed0a2d519bea7550e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "372c6a058c3a484c9af8acbbe5738862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce5a74ea39d4c469f54d0bfb8746023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c18b4b364924a5d913a19afa3f54413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88e584e07af4187a81e1c4973d99b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}